1. Ollama API連携問題とポート競合対応
Ollamaサーバーの起動時に bind エラー発生（ポート11434が svchost.exe に占有されている）

対応策として、ポート転送を11435→11434から11436→11434に変更し .env や netsh で転送設定を再構築

OllamaはWSL2でGPU非対応のためCPUモードでの起動や検証のみ可能

2. LLM選定とモデルダウンロード
日本語対応のOpenHermes-2.5 Q4モデルを選定

モデルダウンロードは huggingface_hub の snapshot_download を使い、Airflowコンテナ外（venv環境）で実行

HuggingFaceのアクセストークンを .env で管理し、スクリプトに渡す形で認証

ダウンロード完了後、モデルファイルはホストの /mnt/e/noctria-kingdom-main/airflow_docker/models/nous-hermes-2 に保存

3. Dockerコンテナ内のモデルマウント問題
docker-compose.yaml のボリュームマウント設定により、ホストのモデルディレクトリをAirflowコンテナの /noctria_kingdom/airflow_docker/models にマウント

.env の HOST_MODELS_PATH と MODEL_DIR 環境変数の整合性を確認

Airflowコンテナ内でのモデルパスは /noctria_kingdom/airflow_docker/models/nous-hermes-2 を使用し、DAGコードにも直書き推奨

マウント問題による FileNotFoundError はパス指定のズレやマウント設定ミスが主因だった

4. DAGコード設計・DB連携
DAGコードではモデルを起動時に一度だけロード（AutoModelForCausalLM.from_pretrained に local_files_only=True を指定）

生成した戦略テキストをPostgreSQLの veritas_outputs テーブルにINSERTする処理を実装

DB接続情報は .env から読み込み、psycopg2 で接続

タスク内で例外処理も実装しエラー時のログ記録を行う

5. Dockerリソース設定
Airflowが動作するDockerに対して、メモリやCPUの割当を適切に設定する重要性を確認

メモリ不足によりタスクが強制終了（return code -9）が発生していた可能性あり

Windows版Docker Desktopの設定画面でメモリ割当を増やす操作を推奨

6. 開発環境（PC・クラウド）検討
開発向けPCは高性能GPU搭載機種（例：RTX 4080搭載ノート）がおすすめ

GPU外付けボックスを使う選択肢もあり

クラウドはAWS、GCP、Azure、Runpodなどを利用可能

Apple M1/M2 MacではTensorFlow Metal対応でGPU活用可能だが、NVIDIA CUDAとは別の環境になる

7. 環境変数・設定ファイル整備
.env にDB接続、モデルパス、ホストパス、HuggingFaceトークンなど必要な変数をまとめる形で運用

docker-compose.yaml では .env を読み込み、各サービス（webserver, scheduler）に環境変数を渡す

PYTHONPATH は /noctria_kingdom/airflow_docker に設定してAirflow内モジュール参照を可能に

8. 次ステップ
DAGの実行テストとログの詳細解析

Dockerリソース設定の見直しとメモリ増強でタスク落ちを防止

モデルのロード・生成処理のパフォーマンス最適化検討

AirflowとDBの連携安定化

さらなるLLM連携やファンダメンタル分析などの機能拡張

この議事録はNoctria KingdomプロジェクトのAI戦略生成基盤構築の現状把握と今後の課題対応に役立ててください。
質問や次のタスクについてはいつでもご連絡ください。
