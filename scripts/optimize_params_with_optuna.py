#!/usr/bin/env python3
# coding: utf-8

from core.path_config import *
from core.logger import setup_logger  # ğŸ° ãƒ­ã‚°çµ±æ²»æ©Ÿæ§‹

import sys
import os
import optuna
import json
from datetime import datetime

from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3 import PPO
from torch.utils.tensorboard import SummaryWriter

from core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals

# âœ… ç‹å›½è¨˜éŒ²ä¿‚ï¼ˆãƒ­ã‚°ï¼‰ã‚’å¬å–š
logger = setup_logger("optimize_logger", LOGS_DIR / "pdca" / "optimize.log")

# âœ… TensorBoard Callback
class TensorBoardCallback(BaseCallback):
    def __init__(self, log_dir, trial_number, verbose=0):
        super().__init__(verbose)
        self.writer = SummaryWriter(log_dir=os.path.join(log_dir, f"trial_{trial_number}"))
        self.episode_rewards = []

    def _on_step(self) -> bool:
        rewards = self.locals.get('rewards', [0])
        if rewards:
            last_reward = rewards[-1]
            self.writer.add_scalar("charts/step_reward", last_reward, self.num_timesteps)
            self.episode_rewards.append(last_reward)
        return True

    def _on_rollout_end(self) -> None:
        if self.episode_rewards:
            episode_return = sum(self.episode_rewards)
            self.writer.add_scalar("charts/episode_return", episode_return, self.num_timesteps)
            self.episode_rewards.clear()

    def _on_training_end(self) -> None:
        self.writer.close()

# âœ… Optuna ç›®çš„é–¢æ•°
def objective(trial):
    logger.info(f"ğŸ¯ è©¦è¡Œ {trial.number} ã‚’é–‹å§‹")

    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    n_steps = trial.suggest_int('n_steps', 128, 2048)
    gamma = trial.suggest_float('gamma', 0.8, 0.9999)
    ent_coef = trial.suggest_float('ent_coef', 0.0, 0.05)

    logger.info(f"ğŸ”§ æ¢ç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: lr={learning_rate}, n_steps={n_steps}, gamma={gamma}, ent_coef={ent_coef}")

    try:
        env = TradingEnvWithFundamentals(DATA_DIR / "preprocessed_usdjpy_with_fundamental.csv")
    except Exception as e:
        logger.error(f"âŒ ç’°å¢ƒåˆæœŸåŒ–å¤±æ•—: {e}")
        raise

    try:
        model = PPO(
            "MlpPolicy", env,
            learning_rate=learning_rate,
            n_steps=n_steps,
            gamma=gamma,
            ent_coef=ent_coef,
            verbose=0,
            tensorboard_log=str(LOGS_DIR / "ppo_tensorboard_logs")
        )

        tb_callback = TensorBoardCallback(str(LOGS_DIR / "ppo_tensorboard_logs"), trial.number)
        model.learn(total_timesteps=1000, callback=tb_callback)

    except Exception as e:
        logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
        raise

    try:
        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)
        logger.info(f"âœ… è©•ä¾¡çµæœï¼ˆå¹³å‡å ±é…¬ï¼‰: {mean_reward}")
        return mean_reward
    except Exception as e:
        logger.error(f"âŒ è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        raise

# âœ… DAGã‚„ä»–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰å‘¼ã¹ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼
def optimize_main():
    study_name = "ppo_opt"
    storage = os.getenv("OPTUNA_DB_URL", "postgresql+psycopg2://airflow:airflow@postgres:5432/optuna_db")

    logger.info(f"ğŸ“š Optuna Studyé–‹å§‹: {study_name}")
    logger.info(f"ğŸ”Œ Storage: {storage}")

    try:
        study = optuna.create_study(
            direction="maximize",
            study_name=study_name,
            storage=storage,
            load_if_exists=True
        )
        study.optimize(objective, n_trials=5)
    except Exception as e:
        logger.error(f"âŒ Optunaæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        raise

    logger.info(f"ğŸ‘‘ æœ€é©ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study.best_params}")

    best_params_file = LOGS_DIR / "best_params.json"
    try:
        with open(best_params_file, "w") as f:
            json.dump(study.best_params, f, indent=2)
        logger.info(f"ğŸ“ ä¿å­˜å®Œäº†: {best_params_file}")
    except Exception as e:
        logger.error(f"âŒ best_params.json ã®ä¿å­˜å¤±æ•—: {e}")
        raise

# âœ… CLIå®Ÿè¡Œæ™‚ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
if __name__ == "__main__":
    optimize_main()
