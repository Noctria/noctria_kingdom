#!/usr/bin/env python3
"""
Auto-fix loop (PDCAå¼·åŒ–ç‰ˆ)
- å¤±æ•—æŒ‡ç´‹(YAML)â†’ä¿®æ­£ã‚¬ã‚¤ãƒ‰æ³¨å…¥
- å¿…é ˆãƒˆãƒ¼ã‚¯ãƒ³æ¤œæŸ»(ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥)
- Few-shot ä¾‹ã®è‡ªå‹•æ³¨å…¥
- æ®µéšãƒ†ã‚¹ãƒˆ(è»½â†’é‡) + å·®åˆ†ãƒ†ã‚¹ãƒˆ
- æˆæœãƒ¡ãƒˆãƒªã‚¯ã‚¹(JSONL)è¨˜éŒ²
- è¿½åŠ : æ¸©åº¦0.0 / ãƒˆãƒ¬ãƒ¼ã‚¹Allowlist / å›å¸°ã‚²ãƒ¼ãƒˆ(å¤±æ•—æ™‚ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯, æˆåŠŸæ™‚ã®ã¿commit)
- è¿½åŠ : pre-commit ãƒã‚§ãƒƒã‚¯çµ±åˆï¼ˆruff-format ç­‰ã®å¤±æ•—ã‚’è‡ªå‹•ä¿®æ­£ï¼‰
- è¿½åŠ : pre-commit å¤±æ•—æ™‚ã¯ .pre-commit-config.yaml ã®å¤‰æ›´ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ãªã„ï¼ˆä¿æŒï¼‰
"""

from __future__ import annotations

import argparse
import json
import os
import re
import shutil
import subprocess
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple

# å…±é€šSP
from codex.prompts.loader import load_noctria_system_prompt

# ===================== è¨­å®š =====================

ROOT = Path(__file__).resolve().parents[1]
ALLOWED_DIRS = ("src/", "tests/")
# è¿½åŠ : ãƒ†ã‚¹ãƒˆå¤–ã®ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ãƒ‘ãƒƒãƒè¨±å¯ï¼ˆpre-commit å¯¾å¿œç”¨ï¼‰
ALLOWED_FILES = (".pre-commit-config.yaml", "pyproject.toml")

MODEL = os.getenv("NOCTRIA_AUTOFIX_MODEL", "gpt-4o-mini")
MAX_ITERS = 5

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ»å‡ºåŠ›ã‚µã‚¤ã‚ºã‚¬ãƒ¼ãƒ‰
CTX_MAX_BYTES = 160_000
MAX_PATCH_BYTES = 300_000
MAX_TOTAL_PATCH_BYTES = 600_000

# å®Ÿè¡Œãƒ•ãƒ©ã‚°
RUN_RUFF = os.getenv("NOCTRIA_AUTOFIX_RUFF", "1") == "1"
RUN_BLACK = os.getenv("NOCTRIA_AUTOFIX_BLACK", "1") == "1"
GIT_COMMIT = os.getenv("NOCTRIA_AUTOFIX_COMMIT", "1") == "1"
LIGHT_FIRST = os.getenv("NOCTRIA_AUTOFIX_LIGHT", "1") == "1"
RUN_PRECOMMIT = os.getenv("NOCTRIA_AUTOFIX_PRECOMMIT", "1") == "1"

# ãƒ–ãƒ©ãƒ³ãƒ
BRANCH_PREFIX = "autofix"

# è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå¸¸ã«æ–‡è„ˆã«å«ã‚ã‚‹ï¼‰
EXTRA_CONTEXT_FILES = [
    "pytest.ini",
    "pyproject.toml",
    ".pre-commit-config.yaml",
    "tests/conftest.py",
    "src/core/path_config.py",
]

# å¤±æ•—æŒ‡ç´‹ãƒ»å¿…é ˆãƒˆãƒ¼ã‚¯ãƒ³ãƒ»Few-shot è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
FINGERPRINTS_YAML = ROOT / "scripts" / "autofix_fingerprints.yaml"
REQUIRED_TOKENS_JSON = ROOT / "scripts" / "autofix_required_tokens.json"
FEWSHOT_DIR = ROOT / "scripts" / "autofix_fewshots"

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜å…ˆ
METRICS_JSONL = ROOT / "src" / "codex_reports" / "autofix_metrics.jsonl"

# âœ… ã“ã“ã§ä¸€åº¦ã ã‘å…±é€šSPã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆcall_model ã‹ã‚‰ä½¿ã†ï¼‰
COMMON_SP = load_noctria_system_prompt("v1.5")

# ===================== ãƒ˜ãƒ«ãƒ‘ =====================


@dataclass
class CmdResult:
    ok: bool
    code: int
    stdout: str
    stderr: str


PytestResult = CmdResult  # åŒå‹


def run(cmd: List[str], check=False, **kw) -> subprocess.CompletedProcess:
    return subprocess.run(cmd, text=True, capture_output=True, check=check, **kw)


def run_pytest(light: bool, target_tests: Optional[List[str]] = None) -> PytestResult:
    """
    env ã§çµã‚Šè¾¼ã¿å¯èƒ½:
      - NOCTRIA_AUTOFIX_PYTEST_LIGHT_ARGS
      - NOCTRIA_AUTOFIX_PYTEST_HEAVY_ARGS
    ä¾‹: export NOCTRIA_AUTOFIX_PYTEST_LIGHT_ARGS='-k "not slow"'
    """
    base = ["pytest", "-q", "--maxfail=1", "--disable-warnings", "-rA"]
    extra = os.getenv(
        "NOCTRIA_AUTOFIX_PYTEST_LIGHT_ARGS" if light else "NOCTRIA_AUTOFIX_PYTEST_HEAVY_ARGS",
        "",
    ).strip()
    args = base[:]
    if extra:
        args += extra.split()
    if target_tests:
        args += target_tests
    proc = run(args, check=False)
    return PytestResult(
        ok=(proc.returncode == 0), code=proc.returncode, stdout=proc.stdout, stderr=proc.stderr
    )


def run_precommit() -> CmdResult:
    try:
        proc = run(["pre-commit", "run", "--all-files", "-v"], check=False)
        return CmdResult(
            ok=(proc.returncode == 0), code=proc.returncode, stdout=proc.stdout, stderr=proc.stderr
        )
    except FileNotFoundError:
        return CmdResult(ok=True, code=0, stdout="(pre-commit not installed)", stderr="")


TB_FILE_RE = re.compile(r'File "([^"]+)", line (\d+), in ([^\n]+)')


def extract_trace_files(stdout: str, stderr: str, limit: int = 12) -> List[Tuple[Path, int, str]]:
    text = "\n".join([stdout[-30000:], stderr[-30000:]])
    hits = TB_FILE_RE.findall(text)
    out: List[Tuple[Path, int, str]] = []
    for fp, line, fn in hits[:limit]:
        p = Path(fp)
        try:
            ln = int(line)
        except Exception:
            ln = 1
        out.append((p, ln, fn))
    return out


def read_snippet(p: Path, around_line: int, radius: int = 36) -> Optional[str]:
    if not p.exists() or not p.is_file():
        return None
    try:
        lines = p.read_text(encoding="utf-8", errors="ignore").splitlines()
    except Exception:
        return None
    i0 = max(0, around_line - radius - 1)
    i1 = min(len(lines), around_line + radius)
    snippet = []
    for idx in range(i0, i1):
        prefix = ">>" if (idx + 1) == around_line else "  "
        snippet.append(f"{prefix}{idx + 1:5d}: {lines[idx]}")
    return "\n".join(snippet)


def collect_repo_context(
    trace_files: List[Tuple[Path, int, str]], extra_files: List[str] | None = None
) -> Dict:
    ctx_items = []
    added: set[str] = set()
    for p, ln, fn in trace_files:
        if not p.exists() or p.suffix not in (".py", ".toml", ".ini", ".cfg", ".yaml", ".yml"):
            continue
        key = str(p)
        if key in added:
            continue
        snippet = read_snippet(p, ln) or ""
        added.add(key)
        ctx_items.append({"path": key, "around_line": ln, "function": fn, "snippet": snippet})
    for extra in extra_files or []:
        pe = Path(extra)
        if pe.exists() and pe.is_file() and str(pe) not in added:
            try:
                txt = pe.read_text(encoding="utf-8", errors="ignore")
            except Exception:
                txt = ""
            ctx_items.append(
                {"path": str(pe), "around_line": 1, "function": "", "snippet": txt[:10000]}
            )
            added.add(str(pe))
    payload = json.dumps(ctx_items, ensure_ascii=False)
    if len(payload.encode("utf-8")) > CTX_MAX_BYTES:
        ctx_items = ctx_items[: max(1, len(ctx_items) // 2)]
    return {"files": ctx_items}


def detect_changed_tests_from_patches(patches: List[Dict]) -> List[str]:
    candidates: set[str] = set()
    for p in patches:
        path = p.get("path", "")
        if not path:
            continue
        if path.startswith("tests/") and path.endswith(".py"):
            candidates.add(path)
        elif path.startswith("src/") and path.endswith(".py"):
            stem = Path(path).stem
            for pat in [f"tests/{stem}*_test.py", f"tests/test_*{stem}*.py"]:
                for m in ROOT.glob(pat):
                    candidates.add(str(m.relative_to(ROOT)))
    return sorted(candidates)


# ===== Allowlist: ãƒˆãƒ¬ãƒ¼ã‚¹ã«å‡ºãŸãƒ‘ã‚¹ã®ã¿è¨±å¯ï¼ˆsrc/, tests/ é…ä¸‹ã«é™ã‚‹ï¼‰ =====


def build_trace_allowlist(trace_files: List[Tuple[Path, int, str]]) -> Set[str]:
    allow: Set[str] = set()
    for p, _, _ in trace_files:
        try:
            rel = str(p.relative_to(ROOT))
        except Exception:
            rel = str(p)
        if any(rel.startswith(d) for d in ALLOWED_DIRS):
            allow.add(rel)
    return allow


# ===================== å¤±æ•—æŒ‡ç´‹ / å¿…é ˆãƒˆãƒ¼ã‚¯ãƒ³ / Few-shot =====================


def load_fingerprints() -> List[Dict]:
    try:
        import yaml  # type: ignore
    except Exception:
        return []
    if not FINGERPRINTS_YAML.exists():
        return []
    try:
        data = yaml.safe_load(FINGERPRINTS_YAML.read_text(encoding="utf-8"))
        return data or []
    except Exception:
        return []


def match_fingerprints(stdout: str, stderr: str) -> List[Dict]:
    fps = load_fingerprints()
    text = "\n".join([stdout[-40000:], stderr[-40000:]])
    out: List[Dict] = []
    for f in fps:
        pat = f.get("when_stdout_regex") or f.get("when_stderr_regex")
        if not pat:
            continue
        try:
            if re.search(pat, text, re.S | re.M):
                out.append(f)
        except re.error:
            pass
    return out


def load_required_tokens() -> Dict[str, List[str]]:
    if not REQUIRED_TOKENS_JSON.exists():
        return {}
    try:
        return json.loads(REQUIRED_TOKENS_JSON.read_text(encoding="utf-8"))
    except Exception:
        return {}


def load_fewshots() -> List[Dict]:
    fewshots: List[Dict] = []
    if not FEWSHOT_DIR.exists():
        return fewshots
    for f in FEWSHOT_DIR.glob("*.jsonl"):
        try:
            for ln in f.read_text(encoding="utf-8").splitlines():
                ln = ln.strip()
                if not ln:
                    continue
                try:
                    obj = json.loads(ln)
                    if isinstance(obj, dict) and "patch" in obj:
                        fewshots.append(obj)
                except Exception:
                    continue
        except Exception:
            continue
    return fewshots[:6]


# ===================== ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ =====================

PROMPT_SYSTEM = """You are a senior Python engineer running an automated test-fixing loop.
You must ONLY return JSON that conforms to the requested schema. Do not include markdown fences or prose.
Constraints:
- Change ONLY files under these directories: {allowed_dirs}.
- Prefer minimal, surgical fixes.
- Output entire-file replacements (new content) instead of partial diffs.
- Keep code safe for offline CI (no network calls unless guarded/stubbed).
- Be careful with indentation, encoding, and imports placement.
- If the root cause is in tests vs src, fix whichever is clearly wrong; otherwise prefer src.
"""

PROMPT_USER_FMT = """Repository failing test summary:
Exit code: {code}

=== Pytest/pre-commit stdout (tail) ===
{stdout_tail}

=== Pytest/pre-commit stderr (tail) ===
{stderr_tail}

Files mentioned in tracebacks (with snippets):
{ctx_json}

Known failure fingerprints and guides (matched):
{fingerprints_json}

Few-shot examples (trimmed):
{fewshots_json}

Task:
- Propose one or more patches to make the failing test(s) pass.
- Return pure JSON with this schema:

{
  "reason": "short explanation",
  "patches": [
    {
      "path": "relative/path/under/src_or_tests.py OR one of {allowed_files}",
      "new_content": "FULL new file content (UTF-8)",
      "why": "what changed"
    }
  ]
}

Rules:
- Only include files under {allowed_dirs} or these files: {allowed_files}.
- Each "new_content" must be the complete new content for that file.
- No placeholders. No backticks.
- If you are not confident, return "patches": [] with a helpful "reason".
"""

# ===================== ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—ï¼ˆå …ç‰¢åŒ– + æ¸©åº¦0.0ï¼‰ =====================


def call_model(user_prompt: str) -> Dict:
    try:
        from openai import OpenAI
    except Exception:
        print("ERROR: openai package not installed. `pip install openai`", file=sys.stderr)
        raise

    client = OpenAI()
    system_msg = COMMON_SP + "\n\n" + PROMPT_SYSTEM.format(allowed_dirs=list(ALLOWED_DIRS))

    # Path A: responses.parse
    try:
        resp = client.responses.parse(
            model=MODEL,
            input=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_prompt},
            ],
            **{"response_format": {"type": "json_object"}},
            temperature=0.0,
        )
        txt = resp.output_text
        return json.loads(txt)
    except Exception:
        pass

    # Path B: chat.completions
    try:
        cc = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.0,
            response_format={"type": "json_object"},
        )
        txt = cc.choices[0].message.content
        return json.loads(txt)
    except Exception:
        pass

    # Path C: responses.createï¼ˆæ‰‹ãƒ‘ãƒ¼ã‚¹ï¼‰
    try:
        resp = client.responses.create(
            model=MODEL,
            temperature=0.0,
            input=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_prompt},
            ],
        )
        txt = getattr(resp, "output_text", None)
        if not txt:
            parts = []
            for item in getattr(resp, "output", []) or []:
                if hasattr(item, "content"):
                    parts.extend(
                        [c.text for c in item.content if getattr(c, "type", "") == "output_text"]
                    )
            txt = "\n".join(parts) if parts else ""
        return json.loads(txt)
    except Exception as e:
        return {"reason": f"model call failed: {e!r}", "patches": []}


# ===================== æ¤œè¨¼ãƒ»é©ç”¨ãƒ»ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ =====================


def validate_patches(
    patches: List[Dict],
    required_tokens: Dict[str, List[str]],
    allowed_paths: Optional[Set[str]] = None,
) -> Tuple[List[Dict], List[str]]:
    errors: List[str] = []
    total_bytes = 0
    valid: List[Dict] = []

    for i, p in enumerate(patches):
        path = p.get("path", "")
        new_content = p.get("new_content", "")
        if not path or not isinstance(path, str):
            errors.append(f"[{i}] missing/invalid path")
            continue

        is_allowed_dir = any(path.startswith(d) for d in ALLOWED_DIRS)
        is_allowed_file = path in ALLOWED_FILES

        if not (is_allowed_dir or is_allowed_file):
            errors.append(f"[{i}] path not allowed: {path}")
            continue

        if allowed_paths is not None and (not is_allowed_file) and (path not in allowed_paths):
            errors.append(f"[{i}] path not in traceback allowlist: {path}")
            continue

        b = len(new_content.encode("utf-8", errors="ignore"))
        if b == 0:
            errors.append(f"[{i}] empty content for {path}")
            continue
        if b > MAX_PATCH_BYTES:
            errors.append(f"[{i}] single patch too large ({b} bytes)")
            continue

        # å¿…é ˆãƒˆãƒ¼ã‚¯ãƒ³æ¤œæŸ»
        req = required_tokens.get(path) or required_tokens.get(str(Path(path)))
        if not req:
            from fnmatch import fnmatch

            for key, toks in required_tokens.items():
                if "*" in key or "?" in key:
                    if fnmatch(path, key):
                        req = toks
                        break
        if req:
            missing = [tok for tok in req if tok not in new_content]
            if missing:
                errors.append(f"[{i}] required tokens missing in {path}: {missing}")
                continue

        total_bytes += b
        valid.append({"path": path, "new_content": new_content, "why": p.get("why", "")})

    if total_bytes > MAX_TOTAL_PATCH_BYTES:
        errors.append(f"total patches too large ({total_bytes} bytes)")
        valid = []
    return valid, errors


def apply_patches_with_backups(patches: List[Dict]) -> List[Tuple[Path, Optional[Path]]]:
    applied: List[Tuple[Path, Optional[Path]]] = []
    for p in patches:
        path = Path(p["path"])
        path.parent.mkdir(parents=True, exist_ok=True)
        backup: Optional[Path] = None
        if path.exists():
            backup = path.with_suffix(path.suffix + ".autofix.bak")
            try:
                shutil.copy2(path, backup)
            except Exception:
                backup = None
        path.write_text(p["new_content"], encoding="utf-8")
        applied.append((path, backup))
    return applied


def format_code() -> None:
    if RUN_RUFF:
        run(["ruff", "check", "--fix", "src", "tests"], check=False)
    if RUN_BLACK:
        run(["black", "src", "tests"], check=False)


def _split_applied(
    applied: List[Tuple[Path, Optional[Path]]],
) -> Tuple[List[Tuple[Path, Optional[Path]]], List[Tuple[Path, Optional[Path]]]]:
    """(.pre-commit-config.yaml / pyproject.toml) ã¨ãã‚Œä»¥å¤–ã«åˆ†å‰²"""
    keep: List[Tuple[Path, Optional[Path]]] = []
    revert: List[Tuple[Path, Optional[Path]]] = []
    allowed = set(ALLOWED_FILES)
    for item in applied:
        p = item[0]
        if p.name in allowed and p.parent == ROOT:
            keep.append(item)
        else:
            revert.append(item)
    return keep, revert


def rollback_files(applied: List[Tuple[Path, Optional[Path]]]) -> None:
    for path, backup in applied:
        try:
            if backup and backup.exists():
                shutil.copy2(backup, path)
                backup.unlink(missing_ok=True)
            else:
                if path.exists():
                    path.unlink()
        except Exception:
            pass


def finalize_backups(applied: List[Tuple[Path, Optional[Path]]]) -> None:
    for _, backup in applied:
        if backup and backup.exists():
            try:
                backup.unlink()
            except Exception:
                pass


def git_commit(message: str) -> None:
    run(["git", "add", "-A"], check=False)
    run(["git", "commit", "-m", message], check=False)


def log_metrics(payload: Dict) -> None:
    try:
        METRICS_JSONL.parent.mkdir(parents=True, exist_ok=True)
        with METRICS_JSONL.open("a", encoding="utf-8") as f:
            f.write(json.dumps(payload, ensure_ascii=False) + "\n")
    except Exception:
        pass


# ===================== ãƒ¡ã‚¤ãƒ³ =====================


def ensure_branch():
    if not GIT_COMMIT:
        return
    ts = time.strftime("%Y%m%d_%H%M%S")
    name = f"{BRANCH_PREFIX}/{ts}"
    base = run(["git", "rev-parse", "--abbrev-ref", "HEAD"]).stdout.strip()
    res = run(["git", "checkout", "-b", name])
    if res.returncode != 0:
        print(f"[warn] could not create branch {name}, staying on {base}")


def _build_user_prompt(stdout_tail: str, stderr_tail: str, ctx: Dict) -> str:
    fewshots = load_fewshots()
    compact_fs = []
    for ex in fewshots:
        compact_fs.append(
            {
                "failure": str(ex.get("failure", ""))[:400],
                "patch": {
                    "path": ex.get("patch", {}).get("path", ""),
                    "why": str(ex.get("patch", {}).get("why", ""))[:200],
                },
            }
        )
    fps = match_fingerprints(stdout_tail, stderr_tail)
    return PROMPT_USER_FMT.format(
        code=0,
        stdout_tail=stdout_tail,
        stderr_tail=stderr_tail,
        ctx_json=json.dumps(ctx, ensure_ascii=False)[:CTX_MAX_BYTES],
        fingerprints_json=json.dumps(fps, ensure_ascii=False),
        fewshots_json=json.dumps(compact_fs, ensure_ascii=False),
        allowed_dirs=list(ALLOWED_DIRS),
        allowed_files=list(ALLOWED_FILES),
    )


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--max-iters", type=int, default=MAX_ITERS)
    args = ap.parse_args()

    ensure_branch()
    required_tokens = load_required_tokens()

    for it in range(1, args.max_iters + 1):
        print(f"\n=== Iteration {it} ===")

        # ã¾ãšè»½ãƒ†ã‚¹ãƒˆ
        start_ts = time.time()
        res_light = run_pytest(light=True)
        light_elapsed = time.time() - start_ts

        if res_light.ok:
            # è»½ãƒ†ã‚¹ãƒˆGREENãªã‚‰é‡ãƒ†ã‚¹ãƒˆï¼ˆå¿…è¦ãªã‚‰ï¼‰
            if LIGHT_FIRST:
                start_ts2 = time.time()
                res_heavy = run_pytest(light=False)
                heavy_elapsed = time.time() - start_ts2
                if res_heavy.ok:
                    # ã“ã“ã§ pre-commit ã‚’å›ã™
                    if RUN_PRECOMMIT:
                        pc = run_precommit()
                        if pc.ok:
                            print("âœ… GREEN â€” all tests & pre-commit passed.")
                            log_metrics(
                                {
                                    "ts": int(time.time()),
                                    "iter": it,
                                    "result": "GREEN",
                                    "model": MODEL,
                                    "light_elapsed_sec": round(light_elapsed, 3),
                                    "heavy_elapsed_sec": round(heavy_elapsed, 3),
                                    "patches": 0,
                                }
                            )
                            return
                        # pre-commit å¤±æ•— â†’ ä¿®æ­£å¯¾è±¡
                        stdout_tail = pc.stdout[-20000:]
                        stderr_tail = pc.stderr[-20000:]
                        trace_files: List[Tuple[Path, int, str]] = []
                    else:
                        print("âœ… GREEN â€” all tests passed.")
                        log_metrics(
                            {
                                "ts": int(time.time()),
                                "iter": it,
                                "result": "GREEN",
                                "model": MODEL,
                                "light_elapsed_sec": round(light_elapsed, 3),
                                "heavy_elapsed_sec": round(heavy_elapsed, 3),
                                "patches": 0,
                            }
                        )
                        return
                else:
                    # é‡ãƒ†ã‚¹ãƒˆã ã‘å¤±æ•— â†’ ä¿®æ­£å¯¾è±¡
                    stdout_tail = res_heavy.stdout[-20000:]
                    stderr_tail = res_heavy.stderr[-20000:]
                    trace_files = extract_trace_files(res_heavy.stdout, res_heavy.stderr)
            else:
                # Light only é‹ç”¨
                if RUN_PRECOMMIT:
                    pc = run_precommit()
                    if pc.ok:
                        print("âœ… GREEN â€” (light tests) & pre-commit passed.")
                        log_metrics(
                            {
                                "ts": int(time.time()),
                                "iter": it,
                                "result": "GREEN_LIGHT_ONLY",
                                "model": MODEL,
                                "light_elapsed_sec": round(light_elapsed, 3),
                                "patches": 0,
                            }
                        )
                        return
                    stdout_tail = pc.stdout[-20000:]
                    stderr_tail = pc.stderr[-20000:]
                    trace_files = []
                else:
                    print("âœ… GREEN â€” (light tests).")
                    log_metrics(
                        {
                            "ts": int(time.time()),
                            "iter": it,
                            "result": "GREEN_LIGHT_ONLY",
                            "model": MODEL,
                            "light_elapsed_sec": round(light_elapsed, 3),
                            "patches": 0,
                        }
                    )
                    return
        else:
            stdout_tail = res_light.stdout[-20000:]
            stderr_tail = res_light.stderr[-20000:]
            trace_files = extract_trace_files(res_light.stdout, res_light.stderr)

        # æ–‡è„ˆåé›† & Allowlist
        ctx = collect_repo_context(trace_files, EXTRA_CONTEXT_FILES)
        trace_allowlist = build_trace_allowlist(trace_files)

        # ãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—
        user_prompt = _build_user_prompt(stdout_tail, stderr_tail, ctx)
        proposal = call_model(user_prompt)
        reason = proposal.get("reason", "")
        patch_list = proposal.get("patches", [])
        print(f"[model] reason: {reason}")
        print(f"[model] num patches: {len(patch_list)}")

        valid, errs = validate_patches(patch_list, required_tokens, allowed_paths=trace_allowlist)
        if errs:
            print("! patch validation errors:")
            for e in errs:
                print("  -", e)
        if not valid:
            print("No valid patches; stopping.")
            log_metrics(
                {
                    "ts": int(time.time()),
                    "iter": it,
                    "result": "NO_VALID_PATCH",
                    "model": MODEL,
                    "errors": errs[-10:],
                }
            )
            sys.exit(2)

        # ===== å›å¸°ã‚²ãƒ¼ãƒˆï¼šé©ç”¨ â†’ æ•´å½¢ â†’ å·®åˆ†ãƒ†ã‚¹ãƒˆ â†’ å¤±æ•—ãªã‚‰ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ =====
        applied = apply_patches_with_backups(valid)
        try:
            format_code()

            # å·®åˆ†ãƒ†ã‚¹ãƒˆ
            target_tests = detect_changed_tests_from_patches(valid)
            start_ts3 = time.time()
            res_diff = run_pytest(light=True, target_tests=target_tests or None)
            diff_elapsed = time.time() - start_ts3

            if not res_diff.ok:
                print("âš ï¸ Diff tests still failing; rolling back this iteration.")
                log_metrics(
                    {
                        "ts": int(time.time()),
                        "iter": it,
                        "result": "DIFF_FAIL",
                        "model": MODEL,
                        "patch_count": len(valid),
                        "diff_elapsed_sec": round(diff_elapsed, 3),
                        "target_tests": target_tests,
                    }
                )
                rollback_files(applied)
                continue

            # è»½ãƒ†ã‚¹ãƒˆå…¨ä½“
            res_light2 = run_pytest(light=True)
            if not res_light2.ok:
                print("âš ï¸ Light tests failed after patch; rolling back.")
                log_metrics(
                    {
                        "ts": int(time.time()),
                        "iter": it,
                        "result": "LIGHT_FAIL",
                        "model": MODEL,
                        "patch_count": len(valid),
                    }
                )
                rollback_files(applied)
                continue

            # é‡ãƒ†ã‚¹ãƒˆï¼ˆå¿…è¦ãªã‚‰ï¼‰
            green = True
            if LIGHT_FIRST:
                res_heavy2 = run_pytest(light=False)
                if not res_heavy2.ok:
                    print("âš ï¸ Heavy tests failed after patch; rolling back.")
                    log_metrics(
                        {
                            "ts": int(time.time()),
                            "iter": it,
                            "result": "HEAVY_FAIL",
                            "model": MODEL,
                            "patch_count": len(valid),
                        }
                    )
                    rollback_files(applied)
                    green = False

            if not green:
                continue

            # pre-commit ã‚‚é€šã™
            if RUN_PRECOMMIT:
                pc2 = run_precommit()
                if not pc2.ok:
                    print("âš ï¸ pre-commit failed after patch; selective rollback (keep config).")
                    # ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼š.pre-commit-config.yaml ç­‰ã¯ä¿æŒã—ã€ãã‚Œä»¥å¤–ã‚’æˆ»ã™
                    keep, revert = _split_applied(applied)
                    rollback_files(revert)
                    finalize_backups(keep)  # config ã® .bak ã¯æƒé™¤
                    log_metrics(
                        {
                            "ts": int(time.time()),
                            "iter": it,
                            "result": "PRECOMMIT_FAIL",
                            "model": MODEL,
                            "patch_count": len(valid),
                            "kept": [str(p[0]) for p in keep],
                            "reverted": [str(p[0]) for p in revert],
                        }
                    )
                    # å¤‰æ›´ã‚’ä¿æŒã—ãŸã¾ã¾æ¬¡ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¸
                    continue

            # ã“ã“ã¾ã§æ¥ãŸã‚‰æˆåŠŸï¼šãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒé™¤ï¼†ã‚³ãƒŸãƒƒãƒˆ
            finalize_backups(applied)
            if GIT_COMMIT:
                git_commit("autofix: apply model patches (green)")

            print("âœ… GREEN â€” all tests (and pre-commit) passed after patch.")
            log_metrics(
                {
                    "ts": int(time.time()),
                    "iter": it,
                    "result": "GREEN_AFTER_PATCH",
                    "model": MODEL,
                    "patch_count": len(valid),
                }
            )
            return

        except Exception as e:
            print(f"âš ï¸ Exception during apply/test: {e}; rolling back.")
            rollback_files(applied)
            continue

    print("ğŸ›‘ Iteration budget exceeded without turning GREEN.")
    log_metrics(
        {"ts": int(time.time()), "iter": MAX_ITERS, "result": "BUDGET_EXCEEDED", "model": MODEL}
    )
    sys.exit(1)


if __name__ == "__main__":
    main()
