#!/usr/bin/env python3
# coding: utf-8

import os
from pathlib import Path
from stable_baselines3 import PPO
from core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals
from core.path_config import MODELS_DIR, DATA_DIR, LOGS_DIR

def evaluate_metaai_model():
    """
    æœ€æ–°ã®MetaAIãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ã€ç´¯ç©å ±é…¬ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ã™ã‚‹ã€‚
    """
    model_path = MODELS_DIR / "latest" / "metaai_model_latest.zip"
    data_path = DATA_DIR / "preprocessed_usdjpy_with_fundamental.csv"
    result_path = LOGS_DIR / "metaai_evaluation_result.txt"

    # === ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª ===
    if not model_path.exists():
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        return

    if not data_path.exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
        return

    print(f"ğŸ“¥ ãƒ¢ãƒ‡ãƒ«èª­è¾¼: {model_path}")
    model = PPO.load(str(model_path))

    # === ç’°å¢ƒåˆæœŸåŒ–ã¨è©•ä¾¡å®Ÿè¡Œ ===
    env = TradingEnvWithFundamentals(str(data_path))
    obs = env.reset()
    total_reward = 0
    done = False

    while not done:
        action, _states = model.predict(obs)
        obs, reward, done, info = env.step(action)
        total_reward += reward

    print(f"âœ… è©•ä¾¡å®Œäº†: ç´¯ç©å ±é…¬ = {total_reward:.2f}")

    # === çµæœãƒ­ã‚°ã«ä¿å­˜ ===
    os.makedirs(LOGS_DIR, exist_ok=True)
    with open(result_path, "w") as f:
        f.write(f"Total reward: {total_reward:.2f}\n")
    print(f"ğŸ“ çµæœä¿å­˜: {result_path}")
