# src/plan_data/collector.py

import os
from datetime import datetime, timedelta
from typing import Optional, Dict
from urllib.parse import urlencode

import pandas as pd
import requests

try:
    import yfinance as yf
except ImportError:
    raise ImportError("yfinance ãŒå¿…è¦ã§ã™: pip install yfinance")

from dotenv import load_dotenv

# .env èª­ã¿è¾¼ã¿ï¼ˆãƒ‘ã‚¹ã¯ç’°å¢ƒã«åˆã‚ã›ã¦ï¼‰
load_dotenv(dotenv_path="/mnt/d/noctria_kingdom/.env")

# FEATURE_SPEC ã¯ãƒªã‚¹ãƒˆï¼ˆæ¨™æº–ã‚«ãƒ©ãƒ é †ï¼‰ã€‚snake_case ã§é‹ç”¨æƒ³å®šã€‚
from plan_data.feature_spec import FEATURE_SPEC  # noqa: E402


ASSET_SYMBOLS: Dict[str, str] = {
    # --- å„ªå…ˆåº¦S, A, Bã‚¢ã‚»ãƒƒãƒˆ ---
    "USDJPY": "JPY=X",
    "SP500": "^GSPC",
    "N225": "^N225",
    "VIX": "^VIX",
    "BTCUSD": "BTC-USD",
    "US10Y": "^TNX",
    "DXY": "DX-Y.NYB",
    "GDAXI": "^GDAXI",
    "FTSE": "^FTSE",
    "EURJPY": "EURJPY=X",
    "EURUSD": "EURUSD=X",
    "AUDJPY": "AUDJPY=X",
    "GBPUSD": "GBPUSD=X",
    "GOLD": "GC=F",
    "WTI": "CL=F",
    "MOVE": "^MOVE",
    "ETHUSD": "ETH-USD",
    "XRPUSD": "XRP-USD",
    "SOLUSD": "SOL-USD",
    "DOGEUSD": "DOGE-USD",
    "HSI": "^HSI",
    "SHANGHAI": "000001.SS",
    "KOSPI": "^KS11",
    "QQQ": "QQQ",
    "TLT": "TLT",
    "GLD": "GLD",
    "RUSSELL": "^RUT",
}

FRED_SERIES: Dict[str, str] = {
    "UNRATE": "UNRATE",
    "FEDFUNDS": "FEDFUNDS",
    "CPI": "CPIAUCSL",
}


def align_to_feature_spec(df: pd.DataFrame) -> pd.DataFrame:
    """
    FEATURE_SPECï¼ˆãƒªã‚¹ãƒˆï¼‰ã«åˆã‚ã›ã¦ä¸è¶³ã‚«ãƒ©ãƒ ã¯è¿½åŠ ï¼ˆNaNï¼‰ã—ã€
    æœ€çµ‚çš„ã«ãã®é †åºã§è¿”ã™ã€‚
    """
    columns = FEATURE_SPEC

    for col in columns:
        if col not in df.columns:
            df[col] = pd.NA

    # å¿…è¦ã§ã‚ã‚Œã°å‹å¤‰æ›ã¯å€‹åˆ¥ã«ï¼ˆã“ã“ã§ã¯è¡Œã‚ãªã„ï¼‰
    return df[columns]


class PlanDataCollector:
    def __init__(self, fred_api_key: Optional[str] = None, event_calendar_csv: Optional[str] = None):
        self.fred_api_key = fred_api_key or os.getenv("FRED_API_KEY")
        if not self.fred_api_key:
            print("[collector] âš ï¸ FRED_API_KEYãŒæœªè¨­å®šã§ã™")
        self.event_calendar_csv = event_calendar_csv or "data/market/event_calendar.csv"

        # ğŸ” NewsAPI â†’ GNews ã¸åˆ‡æ›¿
        self.gnews_key = os.getenv("GNEWS_API_KEY")
        if not self.gnews_key:
            print("[collector] âš ï¸ GNEWS_API_KEYãŒæœªè¨­å®šã§ã™")

    # --------- ãƒ˜ãƒ«ãƒ‘ç¾¤ ---------

    @staticmethod
    def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:
        """
        yfinance ã® MultiIndex åˆ—ãªã©ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã€‚
        ä¾‹: ('Close', 'JPY=X') -> 'Close_JPY=X'
        ã¾ãŸã€éæ–‡å­—åˆ—åˆ—åã¯æ–‡å­—åˆ—åŒ–ã€‚
        """
        cols = df.columns
        if isinstance(cols, pd.MultiIndex):
            df = df.copy()
            df.columns = ["_".join([str(c) for c in tup if c is not None and c != ""])
                          for tup in cols]
            return df
        # å˜ç´”ãªåˆ—ã§ã‚‚ã€å¿µã®ãŸã‚éæ–‡å­—åˆ—ã‚’æ–‡å­—åˆ—åŒ–
        if any(not isinstance(c, str) for c in cols):
            df = df.copy()
            df.columns = [str(c) for c in cols]
        return df

    @staticmethod
    def _ensure_date_column(df: pd.DataFrame) -> pd.DataFrame:
        """
        'date' åˆ—ã‚’ä¿è¨¼ã€‚å­˜åœ¨ã—ãªã‘ã‚Œã° 'Date' / 'Datetime' ç­‰ã‚’æ¢ã—ã¦å¤‰æ›ã€‚
        æ—¢ã« 'date' ãŒã‚ã‚Œã° to_datetime ã ã‘å®Ÿæ–½ã€‚
        """
        df = df.copy()
        candidates = ["date", "Date", "Datetime", "datetime"]
        found = None
        for c in candidates:
            if c in df.columns:
                found = c
                break
        if found is None:
            # yfinance reset_index å¾Œã¯ 'Date' ãŒåŸºæœ¬ã ãŒã€ä¸‡ä¸€ç„¡ã‘ã‚Œã°ãã®ã¾ã¾
            return df
        if found != "date":
            df = df.rename(columns={found: "date"})
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        return df

    @staticmethod
    def _pick_first_matching(df: pd.DataFrame, starts_with: str) -> Optional[str]:
        """
        'Close', 'Close_^GSPC', 'Close_JPY=X' ã®ã‚ˆã†ãªåˆ—ã‹ã‚‰
        starts_withï¼ˆloweræ¯”è¼ƒï¼‰ã§æœ€åˆã«ä¸€è‡´ã™ã‚‹ã‚‚ã®ã‚’è¿”ã™ã€‚
        """
        # å®Œå…¨ä¸€è‡´å„ªå…ˆï¼ˆä¾‹: 'Close' / 'Volume'ï¼‰
        if starts_with in df.columns:
            return starts_with
        low = starts_with.lower()
        for c in df.columns:
            if c.lower().startswith(low):
                return c
        return None

    # --------- ãƒ‡ãƒ¼ã‚¿å–å¾— ---------

    def fetch_multi_assets(
        self,
        start: str,
        end: str,
        interval: str = "1d",
        symbols: Optional[Dict[str, str]] = None,
    ) -> pd.DataFrame:
        """
        yfinance ã§è¤‡æ•°ã‚¢ã‚»ãƒƒãƒˆã® Close/Volume ã‚’å–å¾—ã—ã€date ã§å¤–éƒ¨çµåˆã€‚
        Close/Volume ãŒ MultiIndex ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚‚æŸ”è»Ÿã«æ¤œå‡ºã™ã‚‹ã€‚
        """
        if symbols is None:
            symbols = ASSET_SYMBOLS

        dfs = []
        for key, ticker in symbols.items():
            try:
                df = yf.download(
                    ticker,
                    start=start,
                    end=end,
                    interval=interval,
                    progress=False,
                )
                if df is None or df.empty:
                    print(f"[collector] {ticker} ã®ãƒ‡ãƒ¼ã‚¿ãªã—")
                    continue

                # index -> åˆ—ã¸
                df = df.reset_index()
                # å®‰å…¨ã®ãŸã‚ãƒ•ãƒ©ãƒƒãƒˆåŒ– & date ä¿è¨¼
                df = self._flatten_columns(df)
                df = self._ensure_date_column(df)

                # Close / Volume ã®æŸ”è»Ÿæ¤œå‡º
                close_col = self._pick_first_matching(df, "Close")
                volume_col = self._pick_first_matching(df, "Volume")

                if close_col is None:
                    print(f"[collector] {ticker} ã® Close åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãšã‚¹ã‚­ãƒƒãƒ—: cols={df.columns.tolist()]}")
                    continue

                out_cols = ["date", close_col]
                if volume_col is not None:
                    out_cols.append(volume_col)

                out = df[out_cols].copy().rename(
                    columns={
                        close_col: f"{key.lower()}_close",
                        **({volume_col: f"{key.lower()}_volume"} if volume_col else {}),
                    }
                )

                # æ•°å€¤åŒ–
                for c in out.columns:
                    if c != "date":
                        out[c] = pd.to_numeric(out[c], errors="coerce")

                dfs.append(out)

            except Exception as e:
                print(f"[collector] {ticker} ã®å–å¾—ä¸­ã‚¨ãƒ©ãƒ¼: {e}")

        if not dfs:
            # ç©ºã® DataFrame ã‚’è¿”ã™ï¼ˆå¾Œç¶šã§å®‰å…¨ã«æ‰±ãˆã‚‹ã‚ˆã† 'date' åˆ—ã ã‘æŒãŸã›ã¦ã‚‚OKï¼‰
            return pd.DataFrame(columns=["date"])

        # date ã§é †æ¬¡å¤–éƒ¨çµåˆ
        merged = dfs[0]
        for i in range(1, len(dfs)):
            merged = pd.merge(merged, dfs[i], on="date", how="outer")
            merged = self._ensure_date_column(merged)

        merged = self._ensure_date_column(merged)
        merged = self._flatten_columns(merged)

        if "date" in merged.columns:
            merged = merged.sort_values("date")
            merged = merged.drop_duplicates(subset=["date"], keep="last").reset_index(drop=True)
        else:
            print("[collector] WARNING: 'date' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…¨åˆ—ã§é‡è¤‡æ’é™¤ã‚’å®Ÿæ–½ã€‚")
            merged = merged.drop_duplicates().reset_index(drop=True)

        # å‰æ–¹è£œå®Œï¼ˆçµŒæ¸ˆæŒ‡æ¨™ãªã©é€±æ¬¡ãƒ»æœˆæ¬¡ã¨ä¾¡æ ¼ã®ç²’åº¦å·®ã‚’ãªã‚‰ã™ãŸã‚ï¼‰
        merged = merged.ffill()

        if "usdjpy_close" not in merged.columns:
            print("DEBUG: usdjpy_close ã¯ã‚«ãƒ©ãƒ ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚ç¶šè¡Œã—ã¾ã™ã€‚")

        return merged

    def fetch_fred_data(self, series_id: str, start_date: str, end_date: str) -> pd.DataFrame:
        if not self.fred_api_key:
            print("[collector] âš ï¸ FRED_API_KEYæœªè¨­å®šã€‚FREDãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return pd.DataFrame()
        url = "https://api.stlouisfed.org/fred/series/observations"
        params = {
            "series_id": series_id,
            "api_key": self.fred_api_key,
            "file_type": "json",
            "observation_start": start_date,
            "observation_end": end_date,
        }
        try:
            r = requests.get(url, params=params, timeout=10)
            r.raise_for_status()
            obs = r.json().get("observations", [])
            if not obs:
                return pd.DataFrame()
            df = pd.DataFrame(obs)
            df = df.rename(columns={"date": "date", "value": f"{series_id.lower()}_value"})
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
            df[f"{series_id.lower()}_value"] = pd.to_numeric(df[f"{series_id.lower()}_value"], errors="coerce")
            return df[["date", f"{series_id.lower()}_value"]]
        except Exception as e:
            print(f"[collector] FREDãƒ‡ãƒ¼ã‚¿({series_id})å–å¾—å¤±æ•—: {e}")
            return pd.DataFrame()

    def fetch_event_calendar(self) -> pd.DataFrame:
        """çµŒæ¸ˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼CSVï¼ˆã‚«ãƒ©ãƒ ä¾‹: date, fomc, cpi, nfp, ...ï¼‰"""
        try:
            df = pd.read_csv(self.event_calendar_csv)
            df.columns = [col.lower() for col in df.columns]
            if "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"], errors="coerce")
            return df
        except Exception as e:
            print(f"[collector] ã‚¤ãƒ™ãƒ³ãƒˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼å–å¾—å¤±æ•—: {e}")
            return pd.DataFrame()

    # ===== GNews ç‰ˆï¼ˆæ—¥æ¬¡ä»¶æ•°ã®ç°¡æ˜“é›†è¨ˆï¼‰ =====
    def fetch_gnews_counts(
        self,
        start_date: str,
        end_date: str,
        query: str = '(USD AND JPY) OR ("dollar" AND "yen") OR USDJPY',
        lang: str = "en",
        max_per_day: int = 100,
    ) -> pd.DataFrame:
        """
        GNews v4 ã§æ—¥æ¬¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°ï¼‹ç°¡æ˜“ãƒã‚¸/ãƒã‚¬ä»¶æ•°ã‚’é›†è¨ˆ
        è¿”å´: [date, news_count, news_positive, news_negative]
        """
        api_key = self.gnews_key
        if not api_key:
            print("[collector] GNEWS_API_KEYãŒæœªè¨­å®šã§ã™")
            return pd.DataFrame()

        dt_start = datetime.strptime(start_date, "%Y-%m-%d")
        dt_end = datetime.strptime(end_date, "%Y-%m-%d")
        rows = []

        pos_words = ["gain", "rise", "surge", "record high", "bull", "up", "strong", "beat"]
        neg_words = ["fall", "drop", "crash", "bear", "loss", "down", "weak", "miss"]

        base_url = "https://gnews.io/api/v4/search"

        for d in pd.date_range(dt_start, dt_end):
            day_str = d.strftime("%Y-%m-%d")
            params = {
                "q": query,
                "from": day_str,
                "to": day_str,
                "lang": lang,
                "max": max_per_day,
                "sortby": "publishedAt",
                "token": api_key,
            }
            url = base_url + "?" + urlencode(params)

            try:
                r = requests.get(url, timeout=10)
                r.raise_for_status()
                data = r.json()

                total = data.get("totalArticles", 0)
                articles = data.get("articles", [])

                def get_text(a):
                    t = (a.get("title") or "")
                    dsc = (a.get("description") or "")
                    return (t + " " + dsc).lower()

                pos_count = 0
                neg_count = 0
                for a in articles:
                    text = get_text(a)
                    if any(w in text for w in pos_words):
                        pos_count += 1
                    if any(w in text for w in neg_words):
                        neg_count += 1

                rows.append({
                    "date": pd.to_datetime(day_str),
                    "news_count": int(total),
                    "news_positive": int(pos_count),
                    "news_negative": int(neg_count),
                })
            except Exception as e:
                print(f"[collector] GNews {day_str}å–å¾—å¤±æ•—: {e}")

        return pd.DataFrame(rows) if rows else pd.DataFrame()

    def collect_all(self, lookback_days: int = 365) -> pd.DataFrame:
        end = datetime.today()
        start = end - timedelta(days=lookback_days)
        start_str = start.strftime("%Y-%m-%d")
        end_str = end.strftime("%Y-%m-%d")

        # ãƒãƒ¼ã‚±ãƒƒãƒˆä¾¡æ ¼ç¾¤
        df = self.fetch_multi_assets(start=start_str, end=end_str)

        # FREDï¼ˆCPI/å¤±æ¥­ç‡/FFé‡‘åˆ©ãªã©ï¼‰
        for sid in FRED_SERIES.values():
            fred_df = self.fetch_fred_data(sid, start_str, end_str)
            if not fred_df.empty and df is not None and not df.empty:
                df = pd.merge(df, fred_df, on="date", how="left")
                df = df.sort_values("date").ffill()

        # çµŒæ¸ˆã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼
        event_df = self.fetch_event_calendar()
        if not event_df.empty and df is not None and not df.empty:
            df = pd.merge(df, event_df, on="date", how="left")

        # ğŸ” GNewsï¼ˆæ—¥æ¬¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°ãƒ»ãƒã‚¸ãƒã‚¬ä»¶æ•°ï¼‰
        news_df = self.fetch_gnews_counts(start_str, end_str)
        if not news_df.empty and df is not None:
            if df.empty:
                df = news_df.copy()
            else:
                df = pd.merge(df, news_df, on="date", how="left")

        if df is not None:
            df = df.reset_index(drop=True)
            # collector å´ã§æ¨™æº–ä»•æ§˜ã«æ•´å½¢
            df = align_to_feature_spec(df)

        return df


# --- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¾‹ ---
if __name__ == "__main__":
    collector = PlanDataCollector()
    df = collector.collect_all(lookback_days=7)
    # ãƒ†ã‚¹ãƒˆ: date/newsç³»ã®æœ«å°¾ç¢ºèª
    cols = [c for c in df.columns if "news" in c or c == "date"]
    print(df[cols].tail())
