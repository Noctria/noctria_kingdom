# [NOCTRIA_CORE_REQUIRED]
#!/usr/bin/env python3
# coding: utf-8
"""
ğŸ‘‘ King Noctria (ç†æƒ³å‹ v4.0) - äº”è‡£ã«ã‚ˆã‚‹çµ±æ²» + PM AI (GPT) å…¼ç”¨ç‰ˆ
- æ—¢å­˜Kingã« ModelRouter (Ollamaâ†”GPT) ã‚’éç ´å£Šçµ±åˆ
- ãƒ­ã‚¬ãƒ¼æœªå®šç¾©ã® NameError ã‚’ä¿®æ­£
- äº”è‡£/decideç³»ã¯æœ€å°ã‚¹ã‚¿ãƒ–ã‚’åŒæ¢±ï¼ˆå®Ÿè£…ãŒåˆ¥ã«ã‚ã‚‹å ´åˆã¯è‡ªå‹•ã§å·®æ›¿ or ç½®æ›å¯ï¼‰
"""

from __future__ import annotations

import json
import logging
import os
import re
import uuid
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional

# ----- logger è¨­å®šï¼ˆå…ˆã«å®šç¾©ã—ã¦ NameError é˜²æ­¢ï¼‰ -----
logger = logging.getLogger("noctria.king")
if not logger.handlers:
    _h = logging.StreamHandler()
    _h.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s: %(message)s"))
    logger.addHandler(_h)
logger.setLevel(logging.INFO)

# ---- import çµŒè·¯ã‚’å®‰å®šåŒ– ----
try:
    from src.core.path_config import (
        AIRFLOW_API_BASE,
        ensure_import_path,
        ensure_strategy_packages,
    )
except Exception:
    import sys
    from pathlib import Path

    PROJECT_ROOT = Path(__file__).resolve().parents[2]
    SRC_DIR = PROJECT_ROOT / "src"
    if str(SRC_DIR) not in sys.path:
        sys.path.insert(0, str(SRC_DIR))
    from src.core.path_config import (
        AIRFLOW_API_BASE,
        ensure_import_path,
        ensure_strategy_packages,
    )

ensure_import_path()
ensure_strategy_packages()

# --- ã‚¬ãƒãƒŠãƒ³ã‚¹YAML â†’ System Prompt åˆæˆ / å¤–éƒ¨ä¾å­˜ ---
import requests  # noqa: E402
from src.core.prompt_loader import load_prompt_text, load_governance_dict  # noqa: E402

# --- ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°å±¤ï¼ˆOllamaâ†”GPTï¼‰ ---
try:
    from src.core.model_router import ModelRouter, RouteTask
except Exception:
    ModelRouter = None  # ã¾ã å°å…¥å‰ã§ã‚‚ã‚¨ãƒ©ãƒ¼ã«ã—ãªã„
    RouteTask = None

# -------- ä¾å­˜ï¼ˆOrderExecutionï¼‰ --------
try:
    from src.execution.order_execution import OrderExecution as _OrderExecution
except Exception:
    try:
        from src.do.order_execution import OrderExecution as _OrderExecution
    except Exception:
        _OrderExecution = None


class _NoopOrderExecution:
    def __init__(self, *a, **kw):
        self._api = kw.get("api_url")

    def send(self, order: dict) -> dict:
        logger.info(f"[NOOP-ORDER] would send: {order}")
        return {"status": "noop", "order": order}


OrderExecution = _OrderExecution if _OrderExecution is not None else _NoopOrderExecution


# ----- äº”è‡£ï¼šå®Ÿè£…ãŒã‚ã‚Œã°è‡ªå‹•ã§å·®ã—æ›¿ãˆã€‚ç„¡ã„å ´åˆã¯ã‚¹ã‚¿ãƒ– -----
class _Noop:
    name = "noop"

    def __init__(self, *a, **kw):
        pass

    def __repr__(self):
        return f"<{self.__class__.__name__}>"


try:
    from src.veritas.veritas_machina import VeritasMachina  # å®Ÿè£…ãŒã‚ã‚Œã°ä½¿ã‚ã‚Œã‚‹
except Exception:

    class VeritasMachina(_Noop):
        pass


try:
    from src.strategies.prometheus_oracle import PrometheusOracle
except Exception:

    class PrometheusOracle(_Noop):
        pass


try:
    from src.strategies.aurus_singularis import AurusSingularis
except Exception:

    class AurusSingularis(_Noop):
        pass


try:
    from src.strategies.levia_tempest import LeviaTempest
except Exception:

    class LeviaTempest(_Noop):
        pass


try:
    from src.hermes.strategy_generator import HermesCognitorStrategy
except Exception:

    class HermesCognitorStrategy(_Noop):
        pass


# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: NoctusSentinella ãŒæœªå®šç¾©ãªã‚‰ãƒ€ãƒŸãƒ¼ã«ã™ã‚‹
if "NoctusSentinella" not in globals():

    class NoctusSentinella(_Noop):
        pass


# ------------------- å‹ -------------------
@dataclass
class CouncilResult:
    decision_id: str
    timestamp: str
    final_decision: str
    raw: Dict[str, Any]


# ------------------- OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰ -------------------
def _get_openai_client():
    """
    ã‚ãªãŸã®OpenAIåˆæœŸåŒ–ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«å®Ÿè£…ã—ã¦ã„ã‚Œã°åˆ©ç”¨ã€‚
    æœªè¨­å®šã§ã‚‚ä¾‹å¤–ã«ã›ãš None ã‚’è¿”ã™ã€‚
    """
    try:
        # from openai import OpenAI
        # return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        return None
    except Exception:
        return None


# ------------------- å…±é€šãƒ˜ãƒ«ãƒ‘ -------------------
def _headers_json(api_key: Optional[str] = None) -> Dict[str, str]:
    h = {"Content-Type": "application/json"}
    if api_key:
        h["Authorization"] = f"Bearer {api_key}"
    return h


def _extract_content_from_chat(resp_json: dict) -> str:
    """
    OpenAIäº’æ›ã® /v1/chat/completions ã‹ã‚‰contentã‚’æŠœãã€‚
    ãƒ™ãƒ³ãƒ€å·®ç•°ã«ã‚†ã‚‹ãå¯¾å¿œã€‚
    """
    try:
        return resp_json["choices"][0]["message"]["content"]
    except Exception:
        if "output_text" in resp_json:
            return str(resp_json["output_text"])
        if isinstance(resp_json.get("content"), str):
            return resp_json["content"]
        # OpenAI Responses API äº’æ›ã£ã½ã„å½¢
        if "output" in resp_json and isinstance(resp_json["output"], list):
            parts = []
            for p in resp_json["output"]:
                if isinstance(p, dict) and p.get("type") == "output_text":
                    parts.append(p.get("content", ""))
            if parts:
                return "".join(parts)
        return str(resp_json)[:1000]


# ------------------- ç‹ æœ¬ä½“ -------------------
class KingNoctria:
    def __init__(self):
        logger.info("ç‹ã®è©•è­°ä¼šã‚’æ§‹æˆã™ã‚‹ãŸã‚ã€äº”è‡£ã‚’æ‹›é›†ã—ã¾ã™ã€‚")
        self.veritas = VeritasMachina()
        self.prometheus = PrometheusOracle()
        self.aurus = AurusSingularis()
        self.levia = LeviaTempest()
        self.noctus = NoctusSentinella()
        self.hermes = HermesCognitorStrategy()
        self.order_executor = OrderExecution(api_url="http://host.docker.internal:5001/order")
        logger.info("äº”è‡£ã®æ‹›é›†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        self._agent_cfg = _load_agent_config()

        # PM AIåˆæœŸåŒ–
        self._gov: Optional[dict] = None
        self._system_prompt: Optional[str] = None
        try:
            self._gov = load_governance_dict()  # âœ… dict è¿”ã™
            self._system_prompt = load_prompt_text()  # âœ… æ–‡å­—åˆ—è¿”ã™
        except Exception as e:
            logger.warning(f"ã‚¬ãƒãƒŠãƒ³ã‚¹YAMLèª­è¾¼ã«å¤±æ•—: {e}")
            self._gov = {}
            self._system_prompt = (
                "You are Noctria ç‹.\n"
                "ROLE: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ—ãƒ­ãƒãƒ / æœ€é«˜æ„æ€æ±ºå®šè€…\n"
                "OUTPUT_FORMAT:\n"
                "- Always respond in Japanese.\n"
                "- Return a concise PLAN with numbered steps, plus RISKS and NEXT_ACTIONS.\n"
            )

        self._openai = None
        try:
            self._openai = _get_openai_client()
            logger.info("PM AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            logger.warning(f"PM AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã«å¤±æ•—: {e}")

        # ãƒ«ãƒ¼ã‚¿ãƒ¼ï¼ˆå¿…è¦æ™‚ã«é…å»¶åˆæœŸåŒ–ã‚‚å¯ï¼‰
        self._model_router: Optional[ModelRouter] = ModelRouter() if ModelRouter else None

    # ------------------- æ—¢å­˜ç‹æ”¿ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆå¿…è¦ãªã‚‰ç½®æ›ï¼‰ -------------------
    def decide(self, *a, **kw) -> CouncilResult:
        """
        NOTE: æ—¢å­˜ã®å®Ÿè£…ãŒåˆ¥ã«ã‚ã‚‹å ´åˆã¯ãã¡ã‚‰ã‚’æ®‹ã—ã¦ãã ã•ã„ã€‚
        ã“ã“ã¯è½ã¡ãªã„ãŸã‚ã®æœ€å°å®Ÿè£…ã§ã™ã€‚
        """
        return CouncilResult(
            decision_id=str(uuid.uuid4()),
            timestamp=datetime.now(timezone.utc).isoformat(),
            final_decision="(placeholder) use your existing decide()",
            raw={"note": "replace with project decide() if available"},
        )

    def next_actions(self, *a, **kw) -> List[str]:
        """æœ€å°å®Ÿè£…ã€‚å¿…è¦ãªã‚‰ç½®æ›ã€‚"""
        return ["(placeholder) replace with existing next_actions()"]

    def order_trade(self, order: dict) -> dict:
        """å®Ÿè¡Œã¯ OrderExecution ã«å§”è­²ã€‚"""
        return self.order_executor.send(order)

    def hold_council(self, *a, **kw) -> CouncilResult:
        """æœ€å°å®Ÿè£…ã€‚å¿…è¦ãªã‚‰ç½®æ›ã€‚"""
        return self.decide()

    def _trigger_dag(self, dag_id: str, conf: Optional[dict] = None) -> dict:
        """Airflow DAG ã‚’å©ãã€‚å®Ÿç’°å¢ƒã«åˆã‚ã›ã¦æ‹¡å¼µå¯ã€‚"""
        url = f"{AIRFLOW_API_BASE}/dags/{dag_id}/dagRuns"
        try:
            r = requests.post(url, json={"conf": conf or {}}, timeout=10)
            r.raise_for_status()
            return r.json()
        except Exception as e:
            logger.warning(f"Airflow trigger failed: {e}")
            return {"status": "failed", "reason": str(e)}

    # ------------------- ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°çµ±åˆï¼ˆæ–°è¦ï¼‰ -------------------
    def chat_with_routing(
        self,
        user_prompt: str,
        requires_internet: bool = False,
        category: Optional[str] = None,
        analysis_level: str = "normal",
        hard_model_hint: Optional[str] = None,
        system_prompt: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        æ—¢å­˜ã®ç‹æ”¿ãƒ­ã‚¸ãƒƒã‚¯ã«å½±éŸ¿ã‚’ä¸ãˆãªã„â€œè–„ã„â€ãƒãƒ£ãƒƒãƒˆå…¥å£ã€‚
        ãƒ«ãƒ¼ã‚¿ãƒ¼ã§Ollama/GPTã‚’è‡ªå‹•é¸æŠã—ã€å‘¼ã³å‡ºã—å®Ÿå‡¦ç†ã¯ _call_* ã«å§”è­²ã€‚
        """
        if ModelRouter is None or RouteTask is None:
            raise RuntimeError(
                "ModelRouter/RouteTask ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚model_router.py ã‚’å°å…¥ã—ã¦ãã ã•ã„ã€‚"
            )

        router = self._model_router or ModelRouter()
        self._model_router = router

        task = RouteTask(
            prompt=user_prompt,
            requires_internet=requires_internet,
            category=category,
            analysis_level=analysis_level,
            hard_model_hint=hard_model_hint,
        )
        decision = router.decide_and_prepare(task)

        messages: List[dict] = []
        if system_prompt or self._system_prompt:
            messages.append(
                {"role": "system", "content": system_prompt or self._system_prompt or ""}
            )
        messages.append({"role": "user", "content": user_prompt})

        if decision["provider"] == "ollama":
            out = self._call_ollama(
                base_url=decision["base_url"],
                model=decision["model"],
                messages=messages,
                timeout_s=decision["timeout_s"],
            )
        else:
            out = self._call_gpt(
                base_url=decision["base_url"],
                model=decision["model"],
                api_key=decision.get("api_key") or "",
                messages=messages,
                timeout_s=decision["timeout_s"],
            )

        # èª¬æ˜è²¬ä»»ãƒ­ã‚°
        try:
            self._log_router_decision(
                side=decision["selected_side"],
                model=decision["model"],
                reason=decision["reason"],
                est_tokens=decision["estimated_tokens"],
                prompt_sample=user_prompt[:160],
            )
        except Exception:
            pass

        return {
            "selected_side": decision["selected_side"],
            "model": decision["model"],
            "reason": decision["reason"],
            "estimated_tokens": decision["estimated_tokens"],
            "output": out,
        }

    # ------------------- å®Ÿå‘¼ã³å‡ºã—ï¼ˆå®ŸAPIç‰ˆï¼‰ -------------------
    def _call_ollama(
        self, base_url: str, model: str, messages: List[dict], timeout_s: float
    ) -> Dict[str, Any]:
        """
        1) OpenAIäº’æ› (/v1/chat/completions) ã‚’è©¦è¡Œ
        2) å¤±æ•—æ™‚ã¯ Ollama ãƒã‚¤ãƒ†ã‚£ãƒ– (/api/chat) ã«è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        """
        import requests

        def _as_chat_messages(msgs: List[dict]) -> List[dict]:
            return [{"role": m.get("role", "user"), "content": m.get("content", "")} for m in msgs]

        # ---- 1) OpenAIäº’æ›ã‚’è©¦ã™
        try:
            url_v1 = base_url.rstrip("/") + "/chat/completions"
            payload_v1 = {
                "model": model,
                "messages": _as_chat_messages(messages),
                "temperature": float(os.getenv("OLLAMA_TEMPERATURE", "0.2")),
                "stream": False,
            }
            r = requests.post(
                url_v1,
                json=payload_v1,
                headers=_headers_json(None),
                timeout=float(os.getenv("OLLAMA_TIMEOUT_S", str(timeout_s))),
            )
            if r.status_code < 400:
                data = r.json()
                content = _extract_content_from_chat(data)
                return {
                    "role": "assistant",
                    "content": content,
                    "meta": {"provider": "ollama", "mode": "openai"},
                }
            logger.info(
                f"Ollama openai-compat returned {r.status_code}, falling back to native /api/chat"
            )
        except Exception as e:
            logger.info(f"Ollama openai-compat try failed ({e}); falling back to native /api/chat")

        # ---- 2) ãƒã‚¤ãƒ†ã‚£ãƒ– API (/api/chat)
        try:
            base_native = re.sub(r"/v1/?$", "", base_url.rstrip("/"))
            url_native = base_native + "/api/chat"
            payload_native = {
                "model": model,
                "messages": _as_chat_messages(messages),
                "stream": False,
                "options": {
                    "temperature": float(os.getenv("OLLAMA_TEMPERATURE", "0.2")),
                },
            }
            r2 = requests.post(
                url_native,
                json=payload_native,
                headers={"Content-Type": "application/json"},
                timeout=float(os.getenv("OLLAMA_TIMEOUT_S", str(timeout_s))),
            )
            r2.raise_for_status()
            data2 = r2.json()
            content = (
                data2.get("message", {}).get("content")
                or data2.get("response")
                or _extract_content_from_chat(data2)
            )
            return {
                "role": "assistant",
                "content": content,
                "meta": {"provider": "ollama", "mode": "native"},
            }
        except Exception as e2:
            logger.warning(f"Ollama native call failed: {e2}")
            return {"role": "assistant", "content": f"[Ollama error] {e2}"}

    def _call_gpt(
        self, base_url: str, model: str, api_key: str, messages: List[dict], timeout_s: float
    ) -> Dict[str, Any]:
        """
        OpenAIå…¬å¼/äº’æ›ã® /v1/chat/completions ã‚’ä½¿ç”¨ã€‚
        ï¼ˆResponses APIã§çµ±ä¸€ã—ãŸã„å ´åˆã¯ã“ã“ã‚’æ›¸ãæ›ãˆï¼‰
        """
        url = base_url.rstrip("/") + "/chat/completions"
        payload = {
            "model": model,
            "messages": messages,
            "temperature": float(os.getenv("OPENAI_TEMPERATURE", "0.2")),
            "stream": False,
        }
        try:
            r = requests.post(
                url,
                json=payload,
                headers=_headers_json(api_key),
                timeout=float(os.getenv("OPENAI_TIMEOUT_S", str(timeout_s))),
            )
            r.raise_for_status()
            data = r.json()
            content = _extract_content_from_chat(data)
            return {"role": "assistant", "content": content, "meta": {"provider": "gpt"}}
        except Exception as e:
            logger.warning(f"GPT call failed: {e}")
            return {"role": "assistant", "content": f"[GPT error] {e}"}

    # ------------------- ç›£æŸ»ãƒ­ã‚°ï¼ˆä»»æ„ï¼‰ -------------------
    def _log_router_decision(
        self, side: str, model: str, reason: str, est_tokens: int, prompt_sample: str
    ) -> None:
        record = {
            "ts": datetime.now(timezone.utc).isoformat(),
            "side": side,
            "model": model,
            "reason": reason,
            "estimated_tokens": est_tokens,
            "prompt_sample": prompt_sample,
        }
        try:
            os.makedirs("logs", exist_ok=True)
            path = os.path.join("logs", "king_log.json")
            with open(path, "a", encoding="utf-8") as f:
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.debug(f"router log write failed: {e}")


# ------------------- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®šã®ãƒ­ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜æŒ™å‹•ã‚’è¸è¥²ï¼‰ -------------------
def _load_agent_config() -> dict:
    import os

    try:
        gov = load_governance_dict()  # âœ… dictã§èª­ã¿è¾¼ã‚€
    except Exception:
        return {}
    agent = gov.get("agent", {}) or {}
    tools = gov.get("tools", {}) or {}
    automations = gov.get("automations", {}) or {}
    routing = gov.get("routing", {}) or {}
    memory_policy = gov.get("memory_policy", {}) or {}
    if os.getenv("NOCTRIA_AGENT_ENABLE"):
        agent["enabled"] = os.getenv("NOCTRIA_AGENT_ENABLE") == "1"
    if os.getenv("NOCTRIA_AGENT_MAX_STEPS"):
        agent["max_steps"] = int(os.getenv("NOCTRIA_AGENT_MAX_STEPS"))
    return {
        "agent": agent,
        "tools": tools,
        "automations": automations,
        "routing": routing,
        "memory_policy": memory_policy,
    }


# ------------------- ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œï¼ˆã‚¹ãƒ¢ãƒ¼ã‚¯ï¼‰ -------------------
if __name__ == "__main__":
    try:
        k = KingNoctria()
        demo = k.chat_with_routing(
            "ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®å‹•ä½œç¢ºèªç”¨ã«ã€çŸ­ã„è¿”ç­”ã‚’ãã ã•ã„ã€‚",
            requires_internet=False,
            category="dev",
            analysis_level="light",
        )
        print(json.dumps(demo, ensure_ascii=False, indent=2))
    except Exception as e:
        logger.error(f"chat_with_routing failed: {e}")
