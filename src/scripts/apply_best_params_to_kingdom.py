#!/usr/bin/env python3
# coding: utf-8
from __future__ import annotations

import json
import os
import sys
import shutil
import traceback
from pathlib import Path
from typing import Dict, Any

# å®‰å®š importï¼ˆAirflow/CLI ä¸¡å¯¾å¿œã€src. ã¸çµ±ä¸€ï¼‰
try:
    from src.core.path_config import DATA_DIR, LOGS_DIR
    from src.core.logger import setup_logger
except Exception:
    project_root = Path(__file__).resolve().parents[1]
    sys.path.append(str(project_root))
    from src.core.path_config import DATA_DIR, LOGS_DIR
    from src.core.logger import setup_logger

logger = setup_logger("kingdom_apply_script", LOGS_DIR / "pdca" / "kingdom_apply.log")

def apply_best_params_to_kingdom(
    model_info: Dict[str, Any],
    min_improvement_threshold: float = 0.01,
) -> Dict[str, Any]:
    """
    âœ… æ–°ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ãŒç¾è¡Œãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šè‰¯ã‘ã‚Œã°ã€Œç‹å›½ï¼ˆå…¬å¼ï¼‰ã€ã¸æ˜‡æ ¼ã€‚
    Returns: {"status","detail","error","production_model_path","new_score","old_score"}
    """
    from datetime import datetime

    result: Dict[str, Any] = {
        "status": "ERROR",
        "detail": "",
        "error": "",
        "production_model_path": None,
        "new_score": None,
        "old_score": None,
    }
    try:
        # --- å…¥åŠ›ãƒã‚§ãƒƒã‚¯
        if not model_info or "model_path" not in model_info or "evaluation_score" not in model_info:
            msg = f"ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«æƒ…å ±: {model_info}"
            logger.error("âŒ %s", msg)
            result["detail"] = msg
            return result

        MODELS_DIR = DATA_DIR / "models"                 # path_config v4.6 ã«è¿½éš
        official_dir = MODELS_DIR / "official"
        model_registry_path = official_dir / "model_registry.json"

        new_model_path = Path(model_info["model_path"])
        new_model_score = float(model_info["evaluation_score"])
        result["new_score"] = new_model_score

        logger.info("ğŸ‘‘ æ˜‡æ ¼åˆ¤å®šé–‹å§‹ | å€™è£œ: %s | score=%.4f", new_model_path.name, new_model_score)

        # --- ç¾è¡Œã‚¹ã‚³ã‚¢å–å¾—
        current_score = float("-inf")
        registry = {}
        if model_registry_path.exists():
            try:
                registry = json.loads(model_registry_path.read_text(encoding="utf-8"))
                current_score = float(registry.get("production", {}).get("evaluation_score", float("-inf")))
                logger.info("ç¾è¡Œã‚¹ã‚³ã‚¢: %.4f", current_score)
            except Exception as e:
                logger.warning("âš ï¸ ãƒ¬ã‚¸ã‚¹ãƒˆãƒªèª­ã¿è¾¼ã¿å¤±æ•—ï¼ˆç¶™ç¶šï¼‰: %s", e)
        result["old_score"] = current_score

        # --- ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ã‚²ãƒ¼ãƒˆ
        threshold = current_score + float(min_improvement_threshold)
        if new_model_score < threshold:
            msg = f"æ˜‡æ ¼è¦‹é€ã‚Š: new={new_model_score:.4f} < threshold={threshold:.4f}"
            logger.warning("âš ï¸ %s", msg)
            result["status"] = "SKIPPED"
            result["detail"] = msg
            return result

        # --- ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ & é…ç½®
        if not new_model_path.exists():
            msg = f"æ–°ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {new_model_path}"
            logger.error("âŒ %s", msg)
            result["detail"] = msg
            return result

        official_dir.mkdir(parents=True, exist_ok=True)
        promoted_model_path = official_dir / new_model_path.name
        try:
            shutil.copy2(new_model_path, promoted_model_path)
        except Exception as e:
            err = traceback.format_exc()
            logger.error("ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ”ãƒ¼å¤±æ•—: %s\n%s", e, err)
            result.update(status="ERROR", detail=f"ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ”ãƒ¼å¤±æ•—: {e}", error=err)
            return result

        # --- ãƒ¬ã‚¸ã‚¹ãƒˆãƒªæ›´æ–°
        try:
            history = registry.get("history", []) if registry else []
            if "production" in registry:
                history.insert(0, registry["production"])
                history = history[:10]

            registry_data = {
                "production": {
                    "model_path": str(promoted_model_path.relative_to(MODELS_DIR)),
                    "evaluation_score": new_model_score,
                    "promoted_at": datetime.now().isoformat(),
                },
                "history": history,
            }
            model_registry_path.write_text(json.dumps(registry_data, indent=2), encoding="utf-8")
            logger.info("ğŸ“˜ ãƒ¬ã‚¸ã‚¹ãƒˆãƒªæ›´æ–°: %s", model_registry_path)
        except Exception as e:
            err = traceback.format_exc()
            logger.error("ãƒ¬ã‚¸ã‚¹ãƒˆãƒªæ›´æ–°å¤±æ•—: %s\n%s", e, err)
            result.update(status="ERROR", detail=f"ãƒ¬ã‚¸ã‚¹ãƒˆãƒªæ›´æ–°å¤±æ•—: {e}", error=err)
            return result

        logger.info("ğŸ‰ æ˜‡æ ¼å®Œäº†: %s", promoted_model_path)
        result.update(
            status="SUCCESS",
            detail=f"æ˜‡æ ¼å®Œäº†: {promoted_model_path}",
            production_model_path=str(promoted_model_path),
        )
        return result

    except Exception as e:
        err = traceback.format_exc()
        logger.error("apply_best_params_to_kingdom è‡´å‘½çš„: %s\n%s", e, err)
        result.update(detail=str(e), error=err)
        return result

if __name__ == "__main__":
    # ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
    dummy_dir = DATA_DIR / "models"
    dummy_dir.mkdir(parents=True, exist_ok=True)
    dummy_model = dummy_dir / "metaai_model_dummy.zip"
    dummy_model.touch(exist_ok=True)
    print(json.dumps(apply_best_params_to_kingdom({"model_path": str(dummy_model), "evaluation_score": 1.23}), ensure_ascii=False, indent=2))
