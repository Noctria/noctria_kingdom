#!/usr/bin/env python3
# coding: utf-8

import os
import sys
import json
import optuna
from functools import partial

# Airflowã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‘¼ã³å‡ºã•ã‚ŒãŸå ´åˆã€coreãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ã«ã™ã‚‹
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å˜ä½“ã§å®Ÿè¡Œã™ã‚‹å ´åˆã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’PYTHONPATHã«è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
try:
    from core.path_config import *
    from core.logger import setup_logger
    from core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals
except ImportError:
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã‚’è§£æ±º
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    if project_root not in sys.path:
        sys.path.append(project_root)
    from core.path_config import *
    from core.logger import setup_logger
    from core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback
from optuna.integration.skopt import SkoptSampler
from optuna.pruners import MedianPruner
from optuna.integration import TensorBoardCallback as OptunaTensorBoardCallback # Optunaã®TensorBoardCallback
from optuna_integration.sb3 import TrialPruningCallback # â˜…æ­£ã—ã„ãƒ‘ã‚¹: `optuna_integration`ã‹ã‚‰ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# âœ… ç‹å›½è¨˜éŒ²ä¿‚ï¼ˆãƒ­ã‚°ï¼‰ã‚’å¬å–š
logger = setup_logger("optimize_script", LOGS_DIR / "pdca" / "optimize.log")

# ================================================
# Optuna ç›®çš„é–¢æ•°
# â˜…æ”¹å–„ç‚¹: Pruningã‚’æœ‰åŠ¹ã«ã™ã‚‹ãŸã‚ã€å¼•æ•°ã« trial ã‚’å—ã‘å–ã‚‹
# ================================================
def objective(trial: optuna.Trial, total_timesteps: int, n_eval_episodes: int) -> float:
    logger.info(f"ğŸ¯ è©¦è¡Œ {trial.number} ã‚’é–‹å§‹")

    # --- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ç©ºé–“ã‚’å®šç¾© ---
    params = {
        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),
        'n_steps': trial.suggest_int('n_steps', 128, 2048, step=128),
        'gamma': trial.suggest_float('gamma', 0.9, 0.9999),
        'ent_coef': trial.suggest_float('ent_coef', 0.0, 0.1),
        'clip_range': trial.suggest_float('clip_range', 0.1, 0.4),
    }
    logger.info(f"ğŸ”§ æ¢ç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params}")

    # --- ç’°å¢ƒã¨ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
    try:
        # ç’°å¢ƒã¯å„ãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã§æ–°ã—ãä½œæˆ
        env = TradingEnvWithFundamentals(MARKET_DATA_CSV)
        eval_env = TradingEnvWithFundamentals(MARKET_DATA_CSV) # è©•ä¾¡ç”¨ç’°å¢ƒ
        
        model = PPO("MlpPolicy", env, **params, verbose=0)
    except Exception as e:
        logger.error(f"âŒ ç’°å¢ƒã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å¤±æ•—: {e}", exc_info=True)
        raise optuna.exceptions.TrialPruned() # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°æ‰±ã„

    # --- Pruningç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ---
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=None, # DAGå®Ÿè¡Œä¸­ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ãªã„
        log_path=None, # DAGå®Ÿè¡Œä¸­ã¯ãƒ­ã‚°ã‚’ä¿å­˜ã—ãªã„
        eval_freq=max(total_timesteps // 5, 1), # 5å›ã«åˆ†ã‘ã¦è©•ä¾¡
        deterministic=True,
        render=False,
        callback_after_eval=TrialPruningCallback(trial, "mean_reward") # â˜…è¿½åŠ 
    )

    # --- å­¦ç¿’ã¨è©•ä¾¡ ---
    try:
        model.learn(total_timesteps=total_timesteps, callback=eval_callback)
        mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=n_eval_episodes)
        logger.info(f"âœ… æœ€çµ‚è©•ä¾¡çµæœï¼ˆå¹³å‡å ±é…¬ï¼‰: {mean_reward}")
        return mean_reward
    except (AssertionError, ValueError) as e:
        logger.warning(f"âš ï¸ å­¦ç¿’ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã€è©¦è¡Œã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™: {e}")
        raise optuna.exceptions.TrialPruned()
    except Exception as e:
        logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¾ãŸã¯è©•ä¾¡ã§è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        raise

# ================================================
# DAGã‚„ä»–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰å‘¼ã¹ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
# â˜…æ”¹å–„ç‚¹: Airflow DAGã¨é€£æºã—ã‚„ã™ãä¿®æ­£
# ================================================
def optimize_main(n_trials: int = 20, total_timesteps: int = 20000, n_eval_episodes: int = 10):
    """Optunaã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å®Ÿè¡Œã—ã€æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿”ã™"""
    
    study_name = "noctria_meta_ai_ppo"
    storage = os.getenv("OPTUNA_DB_URL", f"sqlite:///{DATA_DIR / 'optuna_studies.db'}")

    logger.info(f"ğŸ“š Optuna Studyé–‹å§‹: {study_name} (è©¦è¡Œå›æ•°: {n_trials})")
    logger.info(f"ğŸ”Œ Storage: {storage}")

    try:
        # ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¨ãƒ—ãƒ«ãƒ¼ãƒŠãƒ¼ã§ã‚ˆã‚Šè³¢ãæ¢ç´¢
        sampler = SkoptSampler(skopt_kwargs={'base_estimator': 'GP', 'acq_func': 'EI'})
        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=total_timesteps // 3)
        
        study = optuna.create_study(
            direction="maximize",
            study_name=study_name,
            storage=storage,
            sampler=sampler,
            pruner=pruner,
            load_if_exists=True
        )

        # objectiveé–¢æ•°ã«å›ºå®šå¼•æ•°ã‚’æ¸¡ã™ãŸã‚ã«functools.partialã‚’ä½¿ç”¨
        objective_with_params = partial(
            objective,
            total_timesteps=total_timesteps,
            n_eval_episodes=n_eval_episodes
        )

        study.optimize(objective_with_params, n_trials=n_trials, timeout=3600) # 1æ™‚é–“ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        
    except Exception as e:
        logger.error(f"âŒ Optunaæœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return None # ã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneã‚’è¿”ã™

    logger.info("ğŸ‘‘ æœ€é©åŒ–å®Œäº†ï¼")
    logger.info(f"   - æœ€è‰¯è©¦è¡Œ: trial {study.best_trial.number}")
    logger.info(f"   - æœ€é«˜ã‚¹ã‚³ã‚¢: {study.best_value:.4f}")
    logger.info(f"   - æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {study.best_params}")

    # â˜…æ”¹å–„ç‚¹: çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã›ãšã€è¾æ›¸ã¨ã—ã¦è¿”ã™
    return study.best_params

# ================================================
# CLIå®Ÿè¡Œæ™‚ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
# ================================================
if __name__ == "__main__":
    logger.info("CLIã‹ã‚‰ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œã—ã¾ã™ã€‚")
    # å°ã•ãªè¨­å®šã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    best_params = optimize_main(n_trials=5, total_timesteps=1000, n_eval_episodes=2)
    
    if best_params:
        # çµæœã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¦ç¢ºèª
        best_params_file = LOGS_DIR / "best_params_local_test.json"
        with open(best_params_file, "w") as f:
            json.dump(best_params, f, indent=2)
        logger.info(f"ğŸ“ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {best_params_file}")
