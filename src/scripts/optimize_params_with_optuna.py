#!/usr/bin/env python3
# coding: utf-8

"""
Noctria Kingdom - Optuna Hyperparameter Optimization (PPO)
- Gymnasium API æº–æ‹ ã®ç’°å¢ƒã‚’ Monitor ã§åŒ…ã¿ã€DummyVecEnv ã§å­¦ç¿’/è©•ä¾¡ã‚’å®‰å®šåŒ–
- Optuna ã®ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ EvalCallback ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å®Ÿæ–½
- Airflow / ç›´å®Ÿè¡Œ ã®ä¸¡å¯¾å¿œã§ import ã‚’å …ç‰¢åŒ–
"""

from __future__ import annotations

import os
import sys
import json
from functools import partial
from typing import Any

import optuna

# ===== Airflow & CLI ä¸¡å¯¾å¿œã® import è§£æ±ºï¼ˆsrc. ã«çµ±ä¸€ï¼‰ =====
try:
    from src.core.path_config import LOGS_DIR, DATA_DIR, MARKET_DATA_CSV
    from src.core.logger import setup_logger
    from src.core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals
except Exception:
    # æ—§èµ·å‹•/ç›¸å¯¾å®Ÿè¡Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    if project_root not in sys.path:
        sys.path.append(project_root)
    from src.core.path_config import LOGS_DIR, DATA_DIR, MARKET_DATA_CSV  # type: ignore
    from src.core.logger import setup_logger  # type: ignore
    from src.core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals  # type: ignore

LOGS_DIR.mkdir(parents=True, exist_ok=True)
logger = setup_logger("optimize_script", LOGS_DIR / "pdca" / "optimize.log")


# ======================================================
# ğŸ¯ Optunaç”¨ Pruning Callbackï¼ˆEvalCallbackã‚’æ‹¡å¼µï¼‰
#   * è©•ä¾¡ã®ãŸã³ã« last_mean_reward ã‚’ä¸­é–“å€¤ã¨ã—ã¦å ±å‘Š
# ======================================================
from stable_baselines3.common.callbacks import EvalCallback

class OptunaPruningCallback(EvalCallback):
    def __init__(self, eval_env, trial: optuna.Trial, n_eval_episodes=5, eval_freq=1000, verbose=0):
        super().__init__(
            eval_env=eval_env,
            best_model_save_path=None,
            log_path=None,
            eval_freq=eval_freq,
            n_eval_episodes=n_eval_episodes,
            deterministic=True,
            render=False,
            verbose=verbose,
        )
        self.trial = trial
        self.is_pruned = False

    def _on_step(self) -> bool:
        # è¦ªã§è©•ä¾¡å®Ÿè¡Œï¼ˆå¿…è¦ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ evaluate() ãŒå‘¼ã°ã‚Œã‚‹ï¼‰
        cont = super()._on_step()
        # è©•ä¾¡ãŒèµ°ã£ãŸå›ã¯ last_mean_reward ãŒæ›´æ–°ã•ã‚Œã‚‹
        if (self.n_calls % self.eval_freq == 0) and (self.last_mean_reward is not None):
            intermediate_value = float(self.last_mean_reward)
            # ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’ã€Œä¸­é–“ã‚¹ãƒ†ãƒƒãƒ—ã€ã¨ã—ã¦å ±å‘Š
            self.trial.report(intermediate_value, step=self.n_calls)
            if self.trial.should_prune():
                logger.info(f"â© Trial pruned at step={self.n_calls}, reward={intermediate_value:.4f}")
                self.is_pruned = True
                return False  # â†’ å­¦ç¿’åœæ­¢ï¼ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰
        return cont


# ======================================================
# ğŸ¯ Optuna ç›®çš„é–¢æ•°
#   * Monitor + DummyVecEnv ã§åŒ…ã‚€
#   * è©•ä¾¡ã¯ return_episode_rewards=True ã§ãƒ‡ãƒãƒƒã‚°æ€§UP
# ======================================================
def objective(trial: optuna.Trial, total_timesteps: int, n_eval_episodes: int) -> float:
    logger.info(f"ğŸ¯ è©¦è¡Œ {trial.number} ã‚’é–‹å§‹")

    from stable_baselines3 import PPO
    from stable_baselines3.common.monitor import Monitor
    from stable_baselines3.common.vec_env import DummyVecEnv
    from stable_baselines3.common.evaluation import evaluate_policy

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“
    params = {
        "learning_rate": trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True),
        "n_steps": trial.suggest_int("n_steps", 128, 2048, step=128),
        "gamma": trial.suggest_float("gamma", 0.9, 0.9999),
        "ent_coef": trial.suggest_float("ent_coef", 0.0, 0.1),
        "clip_range": trial.suggest_float("clip_range", 0.1, 0.4),
        # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
    }
    logger.info(f"ğŸ”§ æ¢ç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params}")

    # --- ç’°å¢ƒç”Ÿæˆï¼ˆMonitor ã§åŒ…ã‚“ã§ã‹ã‚‰ VecEnv åŒ–ï¼‰ ---
    def make_env():
        env = TradingEnvWithFundamentals(str(MARKET_DATA_CSV))
        return Monitor(env)  # SB3 æ¨å¥¨ã®ãƒ¢ãƒ‹ã‚¿ãƒ¼

    try:
        env = DummyVecEnv([make_env])       # å­¦ç¿’ç”¨
        eval_env = DummyVecEnv([make_env])  # è©•ä¾¡ç”¨
        model = PPO("MlpPolicy", env, **params, verbose=0)
    except Exception as e:
        logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}", exc_info=True)
        raise optuna.TrialPruned()

    # è©•ä¾¡é–“éš”ã¯ç·ã‚¹ãƒ†ãƒƒãƒ—ã® 1/5 ã‚’ç›®å®‰
    eval_freq = max(total_timesteps // 5, 1)
    pruning_cb = OptunaPruningCallback(
        eval_env=eval_env,
        trial=trial,
        n_eval_episodes=n_eval_episodes,
        eval_freq=eval_freq,
        verbose=0,
    )

    # --- å­¦ç¿’ ---
    try:
        model.learn(total_timesteps=total_timesteps, callback=pruning_cb)
        if pruning_cb.is_pruned:
            # ä¸Šã§ False ã‚’è¿”ã—ã¦å­¦ç¿’åœæ­¢ â†’ ã“ã“ã§æ˜ç¤ºçš„ã«ä¼ãˆã‚‹
            raise optuna.TrialPruned()

        # --- è©•ä¾¡ï¼ˆè©³ç´°ãƒ­ã‚°ï¼‰ ---
        mean_reward, ep_rewards = evaluate_policy(
            model,
            eval_env,
            n_eval_episodes=n_eval_episodes,
            return_episode_rewards=True,
            deterministic=True,
        )
        logger.info(f"[DEBUG] ep_returns={ep_rewards}, n={len(ep_rewards)}")
        logger.info(f"âœ… æœ€çµ‚è©•ä¾¡: å¹³å‡å ±é…¬ = {mean_reward:.2f}")

        return float(mean_reward)

    except (AssertionError, ValueError) as e:
        # Gym/Gymnasium ä¸æ•´åˆãªã©ã¯ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã§æ—©æœŸçµ‚äº†
        logger.warning(f"âš ï¸ å­¦ç¿’ä¸­ã®ã‚¨ãƒ©ãƒ¼ã§ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°: {e}")
        raise optuna.TrialPruned()
    except optuna.TrialPruned:
        logger.info("â© ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°åˆ¤å®šã«ã‚ˆã‚Šä¸­æ–­")
        raise
    except Exception as e:
        logger.error(f"âŒ å­¦ç¿’ãƒ»è©•ä¾¡ä¸­ã®è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        # ã“ã“ã¯ â€œå¤±æ•—â€ æ‰±ã„ã«ã—ã¦ãŠãï¼ˆå†ç¾ã®ãŸã‚ raiseï¼‰
        raise
    finally:
        try:
            env.close()
            eval_env.close()
        except Exception:
            pass


# ======================================================
# ğŸš€ DAG / CLI ç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°
# ======================================================
def optimize_main(n_trials: int = 10, total_timesteps: int = 20_000, n_eval_episodes: int = 10) -> dict[str, Any] | None:
    from optuna.integration.skopt import SkoptSampler
    from optuna.pruners import MedianPruner

    study_name = "noctria_meta_ai_ppo"
    storage = os.getenv("OPTUNA_DB_URL")
    if not storage:
        # ãƒ­ãƒ¼ã‚«ãƒ« SQLiteï¼ˆAirflowç„¡ã—ã®ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã§ã‚‚å‹•ãï¼‰
        db_path = DATA_DIR / "optuna_studies.db"
        storage = f"sqlite:///{db_path}"
        logger.warning(f"âš ï¸ OPTUNA_DB_URL æœªè¨­å®šã®ãŸã‚ãƒ­ãƒ¼ã‚«ãƒ«DBã‚’ä½¿ç”¨: {storage}")

    logger.info(f"ğŸ“š Optuna Studyé–‹å§‹: {study_name}")
    logger.info(f"ğŸ”Œ Storage: {storage}")

    try:
        sampler = SkoptSampler(skopt_kwargs={"base_estimator": "GP", "acq_func": "EI"})
        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=max(total_timesteps // 3, 1))

        study = optuna.create_study(
            direction="maximize",
            study_name=study_name,
            storage=storage,
            sampler=sampler,
            pruner=pruner,
            load_if_exists=True,
        )

        objective_with_params = partial(
            objective,
            total_timesteps=total_timesteps,
            n_eval_episodes=n_eval_episodes,
        )

        # timeout ã¯å¿…è¦ãªã‚‰èª¿æ•´
        study.optimize(objective_with_params, n_trials=n_trials, timeout=None)

    except optuna.TrialPruned:
        # ç›´è¿‘ã§ prune ã•ã‚ŒãŸå ´åˆ
        logger.info("â© ç›´è¿‘ Trial ãŒãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã§çµ‚äº†")
    except Exception as e:
        logger.error(f"âŒ Optunaæœ€é©åŒ–ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}", exc_info=True)
        return None

    if len(study.trials) == 0 or study.best_trial is None:
        logger.warning("âš ï¸ æœ‰åŠ¹ãª Trial ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    logger.info("ğŸ‘‘ æœ€é©åŒ–å®Œäº†ï¼")
    logger.info(f"  - Trial: {study.best_trial.number}")
    logger.info(f"  - Score: {study.best_value:.4f}")
    logger.info(f"  - Params: {json.dumps(study.best_params, indent=2)}")
    return study.best_params


# ======================================================
# ğŸ§ª CLI ãƒ‡ãƒãƒƒã‚°ç”¨
# ======================================================
if __name__ == "__main__":
    logger.info("ğŸ§ª CLI: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–‹å§‹")
    best_params = optimize_main(n_trials=5, total_timesteps=2_000, n_eval_episodes=5)

    if best_params:
        best_params_file = LOGS_DIR / "pdca" / "best_params_local_test.json"
        best_params_file.parent.mkdir(parents=True, exist_ok=True)
        with open(best_params_file, "w", encoding="utf-8") as f:
            json.dump(best_params, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ“ ä¿å­˜å®Œäº†: {best_params_file}")
    else:
        logger.info("ï¼ˆbest_params ã¯å¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰")
