#!/usr/bin/env python3
# coding: utf-8
from __future__ import annotations

"""
ğŸ¦‰ Hermes Cognitor (feature_orderæº–æ‹ , API/ãƒ­ãƒ¼ã‚«ãƒ«è‡ªå‹•åˆ‡æ›¿)
- Planå±¤ã®æ¨™æº– dict / feature_order / labels ã‚’è‡ªç„¶è¨€èªèª¬æ˜ã«å¤‰æ›
- decision_id / caller ã‚‚è¿”å´
- ç’°å¢ƒå¤‰æ•°:
  - NOCTRIA_HERMES_MODE=api|offline|auto  (å„ªå…ˆ)
  - HERMES_USE_OPENAI=1                   (å¾Œæ–¹äº’æ›ãƒ•ãƒ©ã‚°ã€ã‚ã‚Œã° api æ‰±ã„)
  - OPENAI_API_KEY=...                    (API åˆ©ç”¨æ™‚ã«å¿…é ˆ)
  - NOCTRIA_OPENAI_MODEL / OPENAI_MODEL   (ãƒ¢ãƒ‡ãƒ«æŒ‡å®šã€æ—¢å®š: gpt-4o-mini)
"""

import os
import json
import logging
from typing import Optional, Dict, Any, List

from src.plan_data.standard_feature_schema import STANDARD_FEATURE_ORDER

# -----------------------------------------------------------------------------
# Loggerï¼ˆAirflow ã‚¿ã‚¹ã‚¯ãƒ­ã‚¬ãƒ¼ã«å¯„ã›ã¤ã¤ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§ã‚‚å‹•ä½œï¼‰
# -----------------------------------------------------------------------------
LOGGER = logging.getLogger("airflow.task")
if not LOGGER.handlers:
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(name)s: %(message)s")

# -----------------------------------------------------------------------------
# ãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
# -----------------------------------------------------------------------------
def _env_mode() -> str:
    """
    Hermes ã®å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’æ±ºå®šã™ã‚‹ã€‚
    - NOCTRIA_HERMES_MODE ãŒå„ªå…ˆï¼ˆoffline/api/autoï¼‰
    - æœªè¨­å®šãªã‚‰ HERMES_USE_OPENAI=1 ã‚’äº’æ›çš„ã«è§£é‡ˆã—ã¦ api
    - ãã‚Œä»¥å¤–ã¯ offline
    """
    mode = (os.getenv("NOCTRIA_HERMES_MODE") or "").strip().lower()
    if mode in {"offline", "api", "auto"}:
        return mode
    if os.getenv("HERMES_USE_OPENAI", "0") == "1":
        return "api"
    return "offline"


# -----------------------------------------------------------------------------
# æœ¬ä½“
# -----------------------------------------------------------------------------
class HermesCognitorStrategy:
    def __init__(
        self,
        model: str = None,
        feature_order: Optional[List[str]] = None,
    ):
        self.model = model or os.getenv("NOCTRIA_OPENAI_MODEL", os.getenv("OPENAI_MODEL", "gpt-4o-mini"))
        self.feature_order = feature_order or STANDARD_FEATURE_ORDER

    # --- Prompt ---------------------------------------------------------------
    def _build_prompt(
        self,
        features: Dict[str, Any],
        labels: List[str],
        feature_order: Optional[List[str]] = None,
        reason: Optional[str] = None,
    ) -> List[Dict[str, str]]:
        """
        OpenAI v1 chat.completions ç”¨ã® messages ã‚’è¿”ã™
        """
        fo = feature_order or self.feature_order

        sys_msg = (
            "ã‚ãªãŸã¯ Noctria ç‹å›½ã®ä¼ä»¤AIã€Hermes Cognitorã€ã§ã™ã€‚"
            "ä¸ãˆã‚‰ã‚ŒãŸæ¨™æº–ç‰¹å¾´é‡ã¨è¦å› ãƒ©ãƒ™ãƒ«ã‹ã‚‰ã€æŠ•è³‡æˆ¦ç•¥ã®æ„å›³ã‚’æ—¥æœ¬èªã§ç°¡æ½”ãƒ»å…·ä½“ã«è¦ç´„ã—ã¾ã™ã€‚"
            "ç®‡æ¡æ›¸ãã§é‡è¦ç‚¹ã‚’3ã€œ6é …ç›®ã€æœ€å¾Œã«1è¡Œã®çµè«–ã‚’æ·»ãˆã¦ãã ã•ã„ã€‚"
        )
        usr_payload = {
            "feature_order": fo,
            "features": features,
            "labels": labels,
            "reason": reason or "",
        }
        user_msg = (
            "ä»¥ä¸‹ã®å…¥åŠ›ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
            "å‡ºåŠ›ã¯ Markdownã€‚è¦‹å‡ºã—â†’ç®‡æ¡æ›¸ãï¼ˆ3ã€œ6é …ç›®ï¼‰â†’æœ€å¾Œã«å¤ªå­—ã®æœ€çµ‚åˆ¤æ–­ã€‚"
            f"\n\nå…¥åŠ›(JSON):\n{json.dumps(usr_payload, ensure_ascii=False, indent=2)}"
        )
        return [
            {"role": "system", "content": sys_msg},
            {"role": "user", "content": user_msg},
        ]

    # --- API çµŒç”± -------------------------------------------------------------
    def _summarize_via_api(
        self,
        features: Dict[str, Any],
        labels: List[str],
        feature_order: Optional[List[str]] = None,
        reason: Optional[str] = None,
    ) -> Optional[str]:
        try:
            from openai import OpenAI  # v1 client
        except Exception:
            LOGGER.warning("[Hermes] openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æœªå°å…¥ã®ãŸã‚ API ã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
            return None

        api_key = os.getenv("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY_NOCTRIA")
        if not api_key:
            LOGGER.warning("[Hermes] OPENAI_API_KEY æœªè¨­å®šã®ãŸã‚ API ã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
            return None

        client = OpenAI(api_key=api_key)
        model = self.model
        msgs = self._build_prompt(features, labels, feature_order, reason)

        try:
            LOGGER.info("[Hermes] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦å®Œäº†")
            resp = client.chat.completions.create(
                model=model,
                messages=msgs,
                temperature=0.2,
                max_tokens=400,
            )
            text = (resp.choices[0].message.content or "").strip()
            # usage ãƒ­ã‚°ï¼ˆã‚ã‚Œã°ï¼‰
            try:
                u = getattr(resp, "usage", None)
                if u:
                    LOGGER.info(
                        "[Hermes] API usage: prompt=%s completion=%s total=%s",
                        getattr(u, "prompt_tokens", None),
                        getattr(u, "completion_tokens", None),
                        getattr(u, "total_tokens", None),
                    )
            except Exception:
                pass
            return text or None
        except Exception as e:
            LOGGER.exception("[Hermes] API å‘¼ã³å‡ºã—ã«å¤±æ•—: %s", e)
            return None

    # --- ãƒ­ãƒ¼ã‚«ãƒ«è¦ç´„ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ ---------------------------------------
    def _summarize_locally(
        self,
        features: Dict[str, Any],
        labels: List[str],
        feature_order: Optional[List[str]] = None,
        reason: Optional[str] = None,
    ) -> str:
        fo = feature_order or self.feature_order
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ†ãƒ³ãƒ—ãƒ¬è¦ç´„ï¼ˆãƒ†ã‚¹ãƒˆã‚„ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ™‚ã®ä»£æ›¿ï¼‰
        bullets: List[str] = []
        if reason:
            bullets.append(f"ç†ç”±ã®è¦æ—¨: {reason[:140]}{'â€¦' if len(reason) > 140 else ''}")
        if labels:
            bullets.append(f"ä¸»å› ãƒ©ãƒ™ãƒ«: {', '.join(labels[:5])}{' ã»ã‹' if len(labels) > 5 else ''}")
        if features:
            # ä»£è¡¨å€¤ã‚’æ•°å€‹ãƒ”ãƒƒã‚¯
            keys = [k for k in fo if k in features] or list(features.keys())
            head = keys[:5]
            preview = ", ".join(f"{k}={features.get(k)!r}" for k in head)
            bullets.append(f"ä¸»è¦ç‰¹å¾´é‡: {preview}{' ã»ã‹' if len(keys) > 5 else ''}")

        if not bullets:
            bullets = ["å…¥åŠ›ç‰¹å¾´é‡ãƒ»ãƒ©ãƒ™ãƒ«ã‹ã‚‰ç‰¹ç­†äº‹é …ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"]

        md = [
            "## å¸‚å ´æˆ¦ç•¥ã®èª¬æ˜ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç”Ÿæˆï¼‰",
            "",
            *[f"- {b}" for b in bullets],
            "",
            "**çµè«–:** æ¡ä»¶ä»˜ãã§æ§˜å­è¦‹ï¼ˆè©³ç´°åˆ¤æ–­ã¯è¿½åŠ ãƒ‡ãƒ¼ã‚¿æ¬¡ç¬¬ï¼‰",
        ]
        return "\n".join(md)

    # --- å¤–éƒ¨ API/ãƒ­ãƒ¼ã‚«ãƒ«ã‚’è‡ªå‹•é¸æŠ ------------------------------------------
    def summarize_strategy(
        self,
        features: Dict[str, Any],
        labels: List[str],
        feature_order: Optional[List[str]] = None,
        reason: Optional[str] = None,
    ) -> str:
        mode = _env_mode()
        LOGGER.info("[Hermes] mode=%s", mode)
        if mode in {"api", "auto"}:
            out = self._summarize_via_api(features, labels, feature_order, reason)
            if out:
                return out
            LOGGER.info("[Hermes] API å¤±æ•—ã®ãŸã‚ãƒ­ãƒ¼ã‚«ãƒ«è¦ç´„ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
        return self._summarize_locally(features, labels, feature_order, reason)

    # --- ãƒ©ãƒƒãƒ‘ï¼ˆPDCA ã‹ã‚‰ä½¿ã„ã‚„ã™ã„å½¢ã§ï¼‰ -----------------------------------
    def propose(
        self,
        input_data: Dict[str, Any],
        decision_id: Optional[str] = None,
        caller: Optional[str] = "king_noctria",
    ) -> Dict[str, Any]:
        features = input_data.get("features", {})
        labels = input_data.get("labels", [])
        feature_order = input_data.get("feature_order", self.feature_order)
        reason = input_data.get("reason", "")

        explanation = self.summarize_strategy(features, labels, feature_order, reason)
        return {
            "name": "HermesCognitor",
            "type": "llm_explanation_report",
            "explanation": explanation,
            "feature_order": feature_order,
            "source_features": features,
            "labels": labels,
            "reason": reason,
            "llm_model": self.model,
            "decision_id": decision_id,
            "caller": caller,
        }

    # --- Markdown åŒ–ï¼ˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã§ä¾¿åˆ©ï¼‰ ------------------------------------
    @staticmethod
    def to_markdown(explanation: str, meta: Optional[Dict[str, Any]] = None) -> str:
        lines = ["# ğŸ¦‰ Hermes Cognitor â€” èª¬æ˜ãƒ¬ãƒãƒ¼ãƒˆ", ""]
        if meta:
            lines += [f"- model: `{meta.get('llm_model')}`",
                      f"- decision_id: `{meta.get('decision_id')}`" if meta.get("decision_id") else "",
                      f"- caller: `{meta.get('caller')}`" if meta.get("caller") else "",
                      ""]
        lines.append(explanation.strip())
        return "\n".join([l for l in lines if l is not None])
