#!/usr/bin/env python3
# coding: utf-8

"""
âœ… Veritas Evaluation Pipeline DAG (v2.1 confå¯¾å¿œ)
- VeritasãŒç”Ÿæˆã—ãŸæˆ¦ç•¥ã‚’å‹•çš„ã«è©•ä¾¡ã—ã€æ¡ç”¨/ä¸æ¡ç”¨ã‚’åˆ¤æ–­ã—ã€ãã®çµæœã‚’è¨˜éŒ²ã™ã‚‹ã€‚
- confï¼ˆç†ç”±ç­‰ï¼‰ã‚’å…¨ã‚¿ã‚¹ã‚¯ã§å—ä¿¡ãƒ»è¨˜éŒ²å¯èƒ½
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
import logging

from airflow.decorators import dag, task
from airflow.operators.python import get_current_context

# --- ãƒ‘ã‚¹èª¿æ•´
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.core.path_config import STRATEGIES_VERITAS_GENERATED_DIR, ACT_LOG_DIR
from src.core.strategy_evaluator import evaluate_strategy, log_evaluation_result

default_args = {
    'owner': 'VeritasCouncil',
    'depends_on_past': False,
    'start_date': datetime(2025, 7, 1),
    'retries': 0,
}

@dag(
    dag_id='veritas_evaluation_pipeline',
    default_args=default_args,
    description='Veritasç”Ÿæˆæˆ¦ç•¥ã®è©•ä¾¡ãƒ»æ¡ç”¨åˆ¤å®šDAGï¼ˆå‹•çš„ã‚¿ã‚¹ã‚¯ãƒ»ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ç‰ˆï¼‰',
    schedule_interval=None,
    catchup=False,
    tags=['veritas', 'evaluation', 'pdca'],
)
def veritas_evaluation_pipeline():
    """
    VeritasãŒç”Ÿæˆã—ãŸæˆ¦ç•¥ã‚’å‹•çš„ã«è©•ä¾¡ã—ã€æ¡ç”¨ã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    confï¼ˆç†ç”±ç­‰ï¼‰ã‚‚å…¨ã‚¿ã‚¹ã‚¯ã§å—ä¿¡ãƒ»è¨˜éŒ²å¯èƒ½
    """

    @task
    def get_strategies_to_evaluate() -> List[str]:
        ctx = get_current_context()
        conf = ctx["dag_run"].conf if ctx.get("dag_run") and ctx["dag_run"].conf else {}
        reason = conf.get("reason", "ç†ç”±æœªæŒ‡å®š")
        logging.info(f"ã€Veritasè©•ä¾¡ã‚¿ã‚¹ã‚¯é–‹å§‹ãƒ»ç™ºä»¤ç†ç”±ã€‘{reason}")

        if not STRATEGIES_VERITAS_GENERATED_DIR.exists():
            logging.warning(f"âš ï¸ æˆ¦ç•¥ç”Ÿæˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {STRATEGIES_VERITAS_GENERATED_DIR}")
            return []
        new_strategies = [
            f.stem for f in STRATEGIES_VERITAS_GENERATED_DIR.iterdir()
            if f.is_file() and f.suffix == '.py' and not f.name.startswith('__')
        ]
        logging.info(f"ğŸ” {len(new_strategies)}ä»¶ã®æ–°ã—ã„æˆ¦ç•¥ã‚’è©•ä¾¡å¯¾è±¡ã¨ã—ã¦ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
        return new_strategies

    @task
    def evaluate_one_strategy(strategy_id: str) -> Dict[str, Any]:
        ctx = get_current_context()
        conf = ctx["dag_run"].conf if ctx.get("dag_run") and ctx["dag_run"].conf else {}
        reason = conf.get("reason", "ç†ç”±æœªæŒ‡å®š")
        logging.info(f"ğŸ“Š è©•ä¾¡é–‹å§‹: {strategy_id}ã€ç™ºä»¤ç†ç”±ã€‘{reason}")
        try:
            result = evaluate_strategy(strategy_id)
            result["status"] = "ok"
            result["trigger_reason"] = reason  # çµæœã«ã‚‚ç†ç”±è¨˜éŒ²
        except Exception as e:
            logging.error(f"ğŸš« è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {strategy_id} âœ {e}", exc_info=True)
            result = {
                "strategy": strategy_id,
                "status": "error",
                "error_message": str(e),
                "trigger_reason": reason
            }
        return result

    @task
    def log_all_results(evaluation_results: List[Dict]):
        ctx = get_current_context()
        conf = ctx["dag_run"].conf if ctx.get("dag_run") and ctx["dag_run"].conf else {}
        reason = conf.get("reason", "ç†ç”±æœªæŒ‡å®š")
        logging.info(f"ğŸ“ {len(evaluation_results) if evaluation_results else 0}ä»¶ã®è©•ä¾¡çµæœã‚’ç‹å›½ã®æ›¸åº«ã«è¨˜éŒ²ã—ã¾ã™â€¦ã€ç™ºä»¤ç†ç”±ã€‘{reason}")
        if not evaluation_results:
            logging.info("è©•ä¾¡å¯¾è±¡ã®æˆ¦ç•¥ãŒãªã‹ã£ãŸãŸã‚ã€ãƒ­ã‚°è¨˜éŒ²ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return

        for result in evaluation_results:
            if result.get("status") == "ok":
                log_evaluation_result(result)
        logging.info("âœ… å…¨ã¦ã®è©•ä¾¡è¨˜éŒ²ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # --- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®šç¾© ---
    strategy_ids = get_strategies_to_evaluate()
    evaluated_results = evaluate_one_strategy.expand(strategy_id=strategy_ids)
    log_all_results(evaluation_results=evaluated_results)

# DAGã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
veritas_evaluation_pipeline()
