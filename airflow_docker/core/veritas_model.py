import os
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# âœ… ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç’°å¢ƒå¤‰æ•°å–å¾—ï¼ˆã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
MODEL_DIR = os.getenv("MODEL_DIR", "/noctria_kingdom/airflow_docker/models/nous-hermes-2")
print(f"ğŸ“¦ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {MODEL_DIR}")

if not os.path.isdir(MODEL_DIR):
    raise FileNotFoundError(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {MODEL_DIR}")

# âœ… GPUç¢ºèªãƒ­ã‚°
if torch.cuda.is_available():
    print(f"ğŸš€ GPUä½¿ç”¨å¯èƒ½: {torch.cuda.get_device_name(0)}")
else:
    print("âš ï¸ GPUãŒä½¿ç”¨ã§ãã¾ã›ã‚“ï¼ˆCPUã§å®Ÿè¡Œã•ã‚Œã¾ã™ï¼‰")

# âœ… ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ï¼ˆdevice_map="auto" ã‚’ä½¿ç”¨ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",                  # â† ã“ã“ãŒé‡è¦
    low_cpu_mem_usage=True,
    local_files_only=True
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)

def generate_fx_strategy(prompt: str) -> str:
    """ç‚ºæ›¿æˆ¦ç•¥ã‚’ç”Ÿæˆ"""
    inputs = tokenizer(prompt, return_tensors="pt")
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_new_tokens=300,
            temperature=0.7,
            top_p=0.95,
            do_sample=True
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# âœ… ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨
if __name__ == "__main__":
    prompt = (
        "ã‚ãªãŸã¯ç‚ºæ›¿å¸‚å ´ã®æˆ¦ç•¥AIã§ã™ã€‚\n"
        "ç¾åœ¨ã®ãƒ‰ãƒ«å††ç›¸å ´ã¯é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‹ã¤æ–¹å‘æ„ŸãŒã‚ã‚Šã¾ã›ã‚“ã€‚\n"
        "çŸ­æœŸã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ã«æœ‰åŠ¹ãªæˆ¦ç•¥ã‚’ã€å…·ä½“çš„ãªãƒ«ãƒ¼ãƒ«ã¨ã—ã¦æç¤ºã—ã¦ãã ã•ã„ã€‚\n"
    )
    result = generate_fx_strategy(prompt)
    print("=== Veritasã®ææ¡ˆ ===\n", result)
