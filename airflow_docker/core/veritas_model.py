import os
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# âœ… ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç’°å¢ƒå¤‰æ•°å–å¾—ï¼ˆã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
MODEL_DIR = os.getenv("MODEL_DIR", "/noctria_kingdom/airflow_docker/models/nous-hermes-2")
print(f"ğŸ“¦ ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {MODEL_DIR}")

if not os.path.isdir(MODEL_DIR):
    raise FileNotFoundError(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {MODEL_DIR}")

# âœ… å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•åˆ¤å®šï¼ˆCPU / CUDAï¼‰
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼èª­ã¿è¾¼ã¿ï¼ˆä½ãƒ¡ãƒ¢ãƒªè¨­å®šï¼‹ãƒ­ãƒ¼ã‚«ãƒ«ã®ã¿ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    MODEL_DIR,
    torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
    low_cpu_mem_usage=True,
    local_files_only=True
).to(device)

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)

def generate_fx_strategy(prompt: str) -> str:
    """ç‚ºæ›¿æˆ¦ç•¥ã‚’ç”Ÿæˆ"""
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_new_tokens=300,
            temperature=0.7,
            top_p=0.95,
            do_sample=True
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# âœ… ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨
if __name__ == "__main__":
    prompt = (
        "ã‚ãªãŸã¯ç‚ºæ›¿å¸‚å ´ã®æˆ¦ç•¥AIã§ã™ã€‚\n"
        "ç¾åœ¨ã®ãƒ‰ãƒ«å††ç›¸å ´ã¯é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‹ã¤æ–¹å‘æ„ŸãŒã‚ã‚Šã¾ã›ã‚“ã€‚\n"
        "çŸ­æœŸã‚¹ã‚­ãƒ£ãƒ«ãƒ”ãƒ³ã‚°ã«æœ‰åŠ¹ãªæˆ¦ç•¥ã‚’ã€å…·ä½“çš„ãªãƒ«ãƒ¼ãƒ«ã¨ã—ã¦æç¤ºã—ã¦ãã ã•ã„ã€‚\n"
    )
    result = generate_fx_strategy(prompt)
    print("=== Veritasã®ææ¡ˆ ===\n", result)
