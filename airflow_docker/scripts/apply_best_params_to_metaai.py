#!/usr/bin/env python3
# coding: utf-8

import os
import json
from core.meta_ai import MetaAI
from core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals
from stable_baselines3 import PPO

def apply_best_params_to_metaai():
    best_params_path = "/opt/airflow/logs/best_params.json"
    
    if not os.path.exists(best_params_path):
        print(f"âŒ æœ€é©åŒ–çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {best_params_path}")
        return

    with open(best_params_path, "r") as f:
        best_params = json.load(f)

    print(f"ğŸ“¦ MetaAI: èª­ã¿è¾¼ã¾ã‚ŒãŸæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")

    # âœ… å­¦ç¿’ç’°å¢ƒã‚’åˆæœŸåŒ–
    data_path = "/opt/airflow/data/preprocessed_usdjpy_with_fundamental.csv"
    env = TradingEnvWithFundamentals(data_path)

    # âœ… MetaAI PPOãƒ¢ãƒ‡ãƒ«å†æ§‹ç¯‰ãƒ»å­¦ç¿’
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=best_params["learning_rate"],
        n_steps=best_params["n_steps"],
        gamma=best_params["gamma"],
        ent_coef=best_params["ent_coef"],
        verbose=1,
        tensorboard_log="/opt/airflow/logs/ppo_tensorboard_logs/"
    )

    print("âš™ï¸ MetaAI: æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    model.learn(total_timesteps=1000)

    # âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆå°†æ¥ã®ãƒ­ãƒ¼ãƒ‰ç”¨ï¼‰
    model.save("/opt/airflow/logs/metaai_model_latest.zip")
    print("âœ… MetaAI: æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨å¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

def main():
    print("ğŸ‘‘ ç‹Noctria: MetaAIã«æœ€é©åŒ–æˆ¦ç•¥ã‚’é©ç”¨ã—ã€ç‹å›½ã®æœªæ¥ã‚’åˆ‡ã‚Šé–‹ã‘ï¼")
    apply_best_params_to_metaai()
    print("ğŸŒŸ ç‹å›½ã®é€²åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼MetaAIã¯æ–°ãŸãªåŠ›ã‚’å¾—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()
