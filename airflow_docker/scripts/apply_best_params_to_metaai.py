#!/usr/bin/env python3
# coding: utf-8

import json
from pathlib import Path

from core.meta_ai import MetaAI
from core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals
from stable_baselines3 import PPO
from core.path_config import LOGS_DIR, DATA_DIR

def apply_best_params_to_metaai():
    # âœ… ãƒ‘ã‚¹ä¸€å…ƒç®¡ç†ï¼ˆNoctria Kingdom v2.0æ§‹æˆï¼‰
    best_params_path = LOGS_DIR / "best_params.json"
    data_path = DATA_DIR / "preprocessed_usdjpy_with_fundamental.csv"
    tensorboard_log_dir = LOGS_DIR / "ppo_tensorboard_logs"
    model_save_path = LOGS_DIR / "metaai_model_latest.zip"

    if not best_params_path.exists():
        print(f"âŒ æœ€é©åŒ–çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {best_params_path}")
        return

    with open(best_params_path, "r") as f:
        best_params = json.load(f)

    print(f"ğŸ“¦ MetaAI: èª­ã¿è¾¼ã¾ã‚ŒãŸæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")

    # âœ… å­¦ç¿’ç’°å¢ƒã‚’åˆæœŸåŒ–ï¼ˆãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«è¾¼ã¿ï¼‰
    env = TradingEnvWithFundamentals(str(data_path))

    # âœ… MetaAI PPOãƒ¢ãƒ‡ãƒ«å†æ§‹ç¯‰ãƒ»å†å­¦ç¿’
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=best_params["learning_rate"],
        n_steps=best_params["n_steps"],
        gamma=best_params["gamma"],
        ent_coef=best_params["ent_coef"],
        verbose=1,
        tensorboard_log=str(tensorboard_log_dir),
    )

    print("âš™ï¸ MetaAI: æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å†å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    model.learn(total_timesteps=1000)

    # âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model.save(str(model_save_path))
    print("âœ… MetaAI: æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©ç”¨å¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")

def main():
    print("ğŸ‘‘ ç‹Noctria: MetaAIã«æœ€é©åŒ–æˆ¦ç•¥ã‚’é©ç”¨ã—ã€ç‹å›½ã®æœªæ¥ã‚’åˆ‡ã‚Šé–‹ã‘ï¼")
    apply_best_params_to_metaai()
    print("ğŸŒŸ ç‹å›½ã®é€²åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼MetaAIã¯æ–°ãŸãªåŠ›ã‚’å¾—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()
