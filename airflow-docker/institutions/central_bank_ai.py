import gym
import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from institutions.central_bank_ai import CentralBankAI  # âœ… è¿½åŠ 

# ğŸ¯ ã‚«ã‚¹ã‚¿ãƒ å ±é…¬é–¢æ•°
def calculate_reward(profit, drawdown, win_rate, recent_profits, cb_score=0.0):
    """
    Noctria Kingdomç‰ˆå ±é…¬é–¢æ•°
    åˆ©ç›Šæœ€å¤§åŒ– + ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³æŠ‘åˆ¶ + å‹ç‡ãƒœãƒ¼ãƒŠã‚¹ + å®‰å®šæ€§ãƒœãƒ¼ãƒŠã‚¹ + ä¸­å¤®éŠ€è¡Œã‚¹ã‚³ã‚¢è£œæ­£
    """
    reward = profit

    max_drawdown_threshold = -30
    if drawdown < max_drawdown_threshold:
        reward += drawdown

    if win_rate > 0.6:
        reward += 10

    # ğŸ¯ å®‰å®šæ€§ãƒœãƒ¼ãƒŠã‚¹: ç›´è¿‘åç›Šã®æ¨™æº–åå·®ãŒå°ã•ã„ã»ã©ãƒœãƒ¼ãƒŠã‚¹
    if len(recent_profits) > 1:
        std_dev = np.std(recent_profits)
        stability_bonus = 5 / (1 + std_dev)
        reward += stability_bonus
    else:
        stability_bonus = 0

    # âœ… ä¸­å¤®éŠ€è¡Œã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹è£œæ­£ï¼ˆä¾‹ï¼šç·©å’Œãªã‚‰+1ã€å¼•ãç· ã‚ãªã‚‰-1ï¼‰
    reward += cb_score * 2

    print(f"å®‰å®šæ€§ãƒœãƒ¼ãƒŠã‚¹: {stability_bonus:.3f} / ä¸­å¤®éŠ€è¡Œã‚¹ã‚³ã‚¢è£œæ­£: {cb_score:.2f}")

    return reward

class MetaAI(gym.Env):
    """
    MetaAI: å„æˆ¦ç•¥AIã‚’çµ±åˆã—ã€å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹è‡ªå·±é€²åŒ–ã‚’è¡Œã†ï¼ˆPPOçµ±åˆç‰ˆãƒ»ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«æ‹¡å¼µç‰ˆï¼‰
    """

    def __init__(self, strategy_agents):
        super(MetaAI, self).__init__()

        self.strategy_agents = strategy_agents

        # âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆOHLCV + ãƒ•ã‚¡ãƒ³ãƒ€ï¼‰
        self.data = pd.read_csv("/opt/airflow/data/preprocessed_usdjpy_with_fundamental.csv", parse_dates=['datetime'])
        self.data.set_index('datetime', inplace=True)
        self.current_step = 0

        # âœ… ä¸­å¤®éŠ€è¡ŒAI
        self.central_bank_ai = CentralBankAI()

        # âœ… è¦³æ¸¬ç©ºé–“
        self.observation_space = gym.spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.data.shape[1],),
            dtype=np.float32
        )

        # âœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“
        self.action_space = gym.spaces.Discrete(3)

        self.ppo_agent = PPO("MlpPolicy", self, verbose=1)

        self.trade_history = []
        self.max_drawdown = 0.0
        self.wins = 0
        self.trades = 0

    def decide_final_action(self, market_state):
        strategy_actions = {
            name: agent.process(market_state)
            for name, agent in self.strategy_agents.items()
        }
        print("å„æˆ¦ç•¥ã®å‡ºåŠ›:", strategy_actions)
        if "SELL" in strategy_actions.values():
            return 2
        elif "BUY" in strategy_actions.values():
            return 1
        else:
            return 0

    def step(self, action):
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1

        obs = self.data.iloc[self.current_step].values.astype(np.float32)

        profit = np.random.uniform(-5, 5)
        spread_cost = np.random.uniform(0, 0.2)
        commission = 0.1

        self.trade_history.append(profit)

        cum_profit = np.cumsum(self.trade_history)
        peak = np.maximum.accumulate(cum_profit)
        drawdowns = peak - cum_profit
        self.max_drawdown = np.max(drawdowns)

        self.trades += 1
        if profit > 0:
            self.wins += 1
        win_rate = self.wins / self.trades if self.trades > 0 else 0.0

        recent_profits = self.trade_history[-10:]

        # âœ… ä¸­å¤®éŠ€è¡Œã‚¹ã‚³ã‚¢ã®ç®—å‡º
        fundamentals = self.data.iloc[self.current_step][["cpi", "interest_diff", "unemployment"]].to_dict()
        cb_score = self.central_bank_ai.evaluate(fundamentals)

        reward = calculate_reward(profit, -self.max_drawdown, win_rate, recent_profits, cb_score)

        print(
            f"Action: {action}, Reward: {reward:.3f}, Drawdown: {self.max_drawdown:.3f}, "
            f"Win Rate: {win_rate:.2f}"
        )

        return obs, reward, done, {}

    def reset(self):
        self.current_step = 0
        self.trade_history.clear()
        self.max_drawdown = 0.0
        self.wins = 0
        self.trades = 0
        obs = self.data.iloc[self.current_step].values.astype(np.float32)
        return obs

    def learn(self, total_timesteps=5000):
        print("=== MetaAI PPOå­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«é–‹å§‹ ===")
        self.ppo_agent.learn(total_timesteps=total_timesteps)
        print("=== MetaAI PPOå­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«å®Œäº† ===")
