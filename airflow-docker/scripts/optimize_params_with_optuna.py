#!/usr/bin/env python3
# /opt/airflow/scripts/optimize_params_with_optuna.py

import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))  # ğŸ”¥ /opt/airflow ã‚’ãƒ‘ã‚¹ã«è¿½åŠ 

import optuna
from core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals
from stable_baselines3 import PPO


def objective(trial):
    # âœ… ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)
    n_steps = trial.suggest_int('n_steps', 64, 2048, step=64)
    gamma = trial.suggest_uniform('gamma', 0.8, 0.9999)
    ent_coef = trial.suggest_uniform('ent_coef', 0.0, 0.05)

    # âœ… ç’°å¢ƒã®åˆæœŸåŒ–
    env = TradingEnvWithFundamentals('/opt/airflow/data/preprocessed_usdjpy_with_fundamental.csv')

    # âœ… PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        gamma=gamma,
        ent_coef=ent_coef,
        verbose=0
    )

    # âœ… å­¦ç¿’å®Ÿè¡Œ
    model.learn(total_timesteps=5000)

    # âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ï¼ˆã“ã“ã§ã¯æœ€çµ‚å ±é…¬ã‚’è¿”ã™ä¾‹ï¼‰
    obs, _ = env.reset()
    total_reward = 0.0
    done = False

    while not done:
        action, _states = model.predict(obs)
        obs, reward, done, _, _ = env.step(action)
        total_reward += reward

    print(f"Trial {trial.number}: total_reward={total_reward}")
    return total_reward


if __name__ == "__main__":
    # âœ… Optunaã®ã‚¹ã‚¿ãƒ‡ã‚£å®šç¾©
    study = optuna.create_study(direction='maximize', study_name='noctria_hyperopt')

    # âœ… æœ€é©åŒ–é–‹å§‹
    study.optimize(objective, n_trials=20)

    # âœ… çµæœå‡ºåŠ›
    print("æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:", study.best_params)
    print("æœ€é«˜å ±é…¬:", study.best_value)
