#!/usr/bin/env python3
import sys
sys.path.append('/opt/airflow')  # âœ… Airflowã‚³ãƒ³ãƒ†ãƒŠã®PYTHONPATHã‚’æ˜ç¤º

import optuna
from core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals
from stable_baselines3 import PPO

def objective(trial):
    # ğŸ¯ Optunaã§è©¦ã™ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    n_steps = trial.suggest_int('n_steps', 128, 2048)
    gamma = trial.suggest_float('gamma', 0.8, 0.9999)
    ent_coef = trial.suggest_float('ent_coef', 0.0, 0.05)

    # âœ… ç’°å¢ƒã®åˆæœŸåŒ–
    env = TradingEnvWithFundamentals('/opt/airflow/data/preprocessed_usdjpy_with_fundamental.csv')

    # âœ… ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        gamma=gamma,
        ent_coef=ent_coef,
        verbose=0
    )

    # âœ… å­¦ç¿’
    model.learn(total_timesteps=1000)  # ãƒ†ã‚¹ãƒˆã®ãŸã‚çŸ­ã‚

    # âœ… ãƒ€ãƒŸãƒ¼ã®è©•ä¾¡æŒ‡æ¨™: ã“ã“ã§ã¯ãƒ†ã‚¹ãƒˆã¨ã—ã¦å›ºå®šã®å€¤ã‚’è¿”ã™ï¼ˆæœ¬æ¥ã¯åç›Šç‡ãªã©ã«ç½®æ›ï¼‰
    mean_reward = 0.0
    return mean_reward

if __name__ == "__main__":
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=10)

    print("æœ€é©ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:", study.best_params)
