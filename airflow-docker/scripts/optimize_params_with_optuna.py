#!/usr/bin/env python3
import sys
import os
import optuna
from datetime import datetime

# âœ… Airflowã‚³ãƒ³ãƒ†ãƒŠã®PYTHONPATHã‚’æ˜ç¤º
sys.path.append('/opt/airflow')

# âœ… TensorBoardã®ãƒ­ã‚¬ãƒ¼è¨­å®š
from stable_baselines3.common.callbacks import BaseCallback
from torch.utils.tensorboard import SummaryWriter

from core.meta_ai_env_with_fundamentals import TradingEnvWithFundamentals
from stable_baselines3 import PPO

# âœ… TensorBoardCallbackã‚¯ãƒ©ã‚¹
class TensorBoardCallback(BaseCallback):
    def __init__(self, log_dir, trial_number, verbose=0):
        super().__init__(verbose)
        self.writer = SummaryWriter(log_dir=os.path.join(log_dir, f"trial_{trial_number}"))

    def _on_step(self) -> bool:
        # ï¼ˆä¾‹ï¼‰stepã”ã¨ã«å ±é…¬ãªã©ã‚’ãƒ€ãƒŸãƒ¼ã§å‡ºåŠ›
        self.writer.add_scalar("charts/reward", 0.0, self.num_timesteps)
        return True

    def _on_training_end(self) -> None:
        self.writer.close()

def objective(trial):
    # ğŸ¯ Optunaã§è©¦ã™ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    n_steps = trial.suggest_int('n_steps', 128, 2048)
    gamma = trial.suggest_float('gamma', 0.8, 0.9999)
    ent_coef = trial.suggest_float('ent_coef', 0.0, 0.05)

    # âœ… ç’°å¢ƒã®åˆæœŸåŒ–
    env = TradingEnvWithFundamentals('/opt/airflow/data/preprocessed_usdjpy_with_fundamental.csv')

    # âœ… ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        gamma=gamma,
        ent_coef=ent_coef,
        verbose=0,
        tensorboard_log="/opt/airflow/logs/ppo_tensorboard_logs/"
    )

    # âœ… ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆTensorBoardã«å„trialã”ã¨ã®ãƒ­ã‚°ã‚’è¨˜éŒ²ï¼‰
    tb_callback = TensorBoardCallback("/opt/airflow/logs/ppo_tensorboard_logs", trial.number)

    # âœ… å­¦ç¿’
    model.learn(total_timesteps=1000, callback=tb_callback)  # 1000ã¯ãƒ†ã‚¹ãƒˆç”¨

    # âœ… ãƒ€ãƒŸãƒ¼ã®è©•ä¾¡æŒ‡æ¨™: ã“ã“ã§ã¯å›ºå®šã®å€¤ã‚’è¿”ã™ï¼ˆæœ¬ç•ªã§ã¯åç›Šç‡ãªã©ã«ç½®æ›ï¼‰
    mean_reward = 0.0
    return mean_reward

if __name__ == "__main__":
    # âœ… Optunaã®ã‚¹ã‚¿ãƒ‡ã‚£åãƒ»ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®š
    study_name = "ppo_hyperparam_optimization"

    # SQLiteã‚’ä½¿ã†å ´åˆï¼ˆAirflowã‚³ãƒ³ãƒ†ãƒŠå†…ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼‰
    # storage = f"sqlite:////opt/airflow/logs/{study_name}.db"

    # Postgresã‚’ä½¿ã†å ´åˆï¼ˆä¾‹: airflowå†…ã®postgresã‚’åˆ©ç”¨ï¼‰
    storage = f"postgresql+psycopg2://airflow:airflow@postgres/optuna_db"

    # âœ… studyã‚’ä½œæˆãƒ»æ—¢å­˜ãŒã‚ã‚Œã°ãƒ­ãƒ¼ãƒ‰
    study = optuna.create_study(
        direction="maximize",
        study_name=study_name,
        storage=storage,
        load_if_exists=True
    )

    # âœ… æœ€é©åŒ–å®Ÿè¡Œ
    study.optimize(objective, n_trials=10)

    print("æœ€é©ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:", study.best_params)
