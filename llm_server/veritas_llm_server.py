#!/usr/bin/env python3
# coding: utf-8

"""
ğŸ§  Veritas LLM Server (v2.0)
- LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€æˆ¦ç•¥ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¿œç­”ã™ã‚‹APIã‚µãƒ¼ãƒãƒ¼
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚®ãƒ³ã‚°ã‚’å¼·åŒ–
"""

import os
import torch
import time
import logging
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
from dotenv import load_dotenv
from typing import Dict

# --- ç‹å›½ã®åŸºç›¤ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›¸å¯¾ãƒ‘ã‚¹ã‹ã‚‰llm_prompt_builderã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from llm_prompt_builder import load_strategy_template

# --- ãƒ­ã‚¬ãƒ¼ã¨ç’°å¢ƒè¨­å®š ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [%(levelname)s] - %(message)s')
load_dotenv()

# --- ãƒ¢ãƒ‡ãƒ«è¨­å®š ---
MODEL_PATH = os.getenv("MODEL_DIR", "/path/to/your/default/model") # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’è¨­å®š

# --- LLMã‚µãƒ¼ãƒ“ã‚¹ã®ã‚«ãƒ—ã‚»ãƒ«åŒ– ---
class VeritasLLMService:
    """LLMãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨æ¨è«–å‡¦ç†ã‚’ç®¡ç†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹"""
    def __init__(self, model_path: str):
        self.tokenizer = None
        self.model = None
        self._load_model(model_path)

    def _load_model(self, model_path: str):
        """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€"""
        logging.info("Veritasã®çŸ¥æ€§ã®æºæ³‰ã€LLMã®å¬å–šæº–å‚™ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
        if not os.path.exists(model_path):
            logging.error(f"è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã®æ ¼ç´å ´æ‰€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
            raise RuntimeError(f"Model directory not found: {model_path}")

        logging.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’å¬å–šä¸­: {model_path}")
        torch.cuda.empty_cache()

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto",
                trust_remote_code=True
            )
            self.model.eval()
            logging.info("LLMã®å¬å–šã«æˆåŠŸã€‚Veritasã¯æ€è€ƒã‚’é–‹å§‹ã§ãã¾ã™ã€‚")
        except Exception as e:
            logging.error(f"LLMã®å¬å–šä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
            raise

# --- FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ– ---
app = FastAPI(
    title="Veritas LLM Server",
    description="Noctriaç‹å›½ã®ãŸã‚ã«æ–°ãŸãªæˆ¦ç•¥ã‚’ç”Ÿæˆã™ã‚‹AIã‚µãƒ¼ãƒãƒ¼",
    version="2.0"
)

# LLMã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
try:
    llm_service = VeritasLLMService(MODEL_PATH)
    strategy_template = load_strategy_template()
except RuntimeError as e:
    logging.critical(f"ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    # ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã›ãšã«çµ‚äº†ã•ã›ã‚‹ã‹ã€ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã‚’ç¤ºã™
    llm_service = None 
    strategy_template = "ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"


# --- APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å®šç¾© ---
class PromptRequest(BaseModel):
    prompt: str
    max_new_tokens: int = 256
    temperature: float = 0.8
    top_p: float = 0.95
    do_sample: bool = True

class GenerationResponse(BaseModel):
    response: str
    elapsed_time: float
    input_length: int

# --- APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®å®šç¾© ---
@app.get("/")
def root():
    """ã‚µãƒ¼ãƒãƒ¼ã®ç¨¼åƒçŠ¶æ…‹ã‚’ç¢ºèªã™ã‚‹ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    status = "ç¨¼åƒä¸­" if llm_service else "èµ·å‹•å¤±æ•—"
    return {"message": f"ğŸ§  Veritas LLMã‚µãƒ¼ãƒãƒ¼ {status}ï¼ˆgpusorobanåŸºç›¤ï¼‰"}

@app.post("/generate", response_model=GenerationResponse)
def generate(req: PromptRequest):
    """æˆ¦ç•¥ç”Ÿæˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å—ã‘å–ã‚Šã€LLMã«ã‚ˆã‚‹ç”Ÿæˆçµæœã‚’è¿”ã™"""
    if not llm_service:
        raise HTTPException(status_code=503, detail="LLMã‚µãƒ¼ãƒ“ã‚¹ãŒåˆ©ç”¨ä¸å¯èƒ½ã§ã™ã€‚")

    start_time = time.time()
    logging.info(f"æˆ¦ç•¥ç”Ÿæˆã®ç¥è¨—ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚æŒ‡ç¤º: ã€Œ{req.prompt[:50]}...ã€")

    full_prompt = f"""ã‚ãªãŸã¯AIæˆ¦ç•¥ç”Ÿæˆè€…Veritasã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«æº–æ‹ ã—ãŸå½¢å¼ã§ã€æ–°ã—ã„æˆ¦ç•¥ã‚’Pythonã‚³ãƒ¼ãƒ‰ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

--- æˆ¦ç•¥ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ ---
{strategy_template}

--- ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤º ---
{req.prompt}
"""
    try:
        inputs = llm_service.tokenizer(
            full_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(llm_service.model.device)

        outputs = llm_service.model.generate(
            **inputs,
            max_new_tokens=req.max_new_tokens,
            temperature=req.temperature,
            top_p=req.top_p,
            do_sample=req.do_sample,
            pad_token_id=llm_service.tokenizer.eos_token_id # warningæŠ‘åˆ¶
        )

        result = llm_service.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºï¼ˆã‚ˆã‚Šå …ç‰¢ãªæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹ï¼‰
        # ã“ã“ã§ã¯å˜ç´”ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéƒ¨åˆ†ã‚’å‰Šé™¤
        response_text = result[len(full_prompt):].strip()

    except Exception as e:
        logging.error(f"æˆ¦ç•¥ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="æˆ¦ç•¥ã®ç”Ÿæˆä¸­ã«å†…éƒ¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

    elapsed = round(time.time() - start_time, 2)
    logging.info(f"æ€è€ƒæ™‚é–“: {elapsed}ç§’ | å…¥åŠ›æ–‡å­—æ•°: {len(full_prompt)}")

    return {
        "response": response_text,
        "elapsed_time": elapsed,
        "input_length": len(full_prompt)
    }
