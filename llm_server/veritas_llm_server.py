# veritas_llm_server.py

import os
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# === ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ä¸€åº¦ã ã‘ï¼‰ ===
MODEL_DIR = os.getenv("MODEL_DIR", "/noctria_kingdom/airflow_docker/models/nous-hermes-2")

if not os.path.exists(MODEL_DIR):
    raise RuntimeError(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {MODEL_DIR}")

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)
model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, local_files_only=True)

# === FastAPI åˆæœŸåŒ– ===
app = FastAPI(title="Veritas LLM Server")

class PromptRequest(BaseModel):
    prompt: str
    max_tokens: int = 300

@app.post("/run_veritas_llm")
def generate_response(data: PromptRequest):
    try:
        inputs = tokenizer(data.prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(inputs["input_ids"], max_new_tokens=data.max_tokens)
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return {"response": result.strip()}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ğŸš¨ æ¨è«–ã‚¨ãƒ©ãƒ¼: {str(e)}")

@app.get("/")
def healthcheck():
    return {"status": "ğŸ§  Veritas LLMã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­"}

# === ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ç”¨ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ ===
if __name__ == "__main__":
    uvicorn.run("veritas_llm_server:app", host="0.0.0.0", port=8000)
