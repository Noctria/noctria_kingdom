from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from pathlib import Path

app = FastAPI()

# ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰
model_path = Path("/mnt/e/noctria-kingdom-main/airflow_docker/models/openchat-3.5").resolve()
print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {model_path}")

# ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª
if not model_path.is_dir():
    raise ValueError(f"âŒ æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {model_path}")

# ğŸ”„ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«é™å®šï¼‰
tokenizer = AutoTokenizer.from_pretrained(
    model_path,
    trust_remote_code=True,
    local_files_only=True
)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    trust_remote_code=True,
    local_files_only=True,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
)

# ãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«
model.eval()
if torch.cuda.is_available():
    model = model.to("cuda")

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¹ã‚­ãƒ¼ãƒ
class PromptRequest(BaseModel):
    prompt: str
    max_tokens: int = 256
    temperature: float = 0.7

@app.post("/generate")
async def generate_text(req: PromptRequest):
    try:
        inputs = tokenizer(req.prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}

        outputs = model.generate(
            **inputs,
            max_new_tokens=req.max_tokens,
            temperature=req.temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return {"response": decoded}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
