import logging
import os
import json
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
from pathlib import Path
from dotenv import load_dotenv

logging.info(
    "LLM flags: enabled=%s mode=%s base=%s model=%s",
    os.getenv("NOCTRIA_LLM_ENABLED"),
    os.getenv("NOCTRIA_HARMONIA_MODE"),
    os.getenv("OPENAI_API_BASE") or os.getenv("OPENAI_BASE_URL"),
    os.getenv("NOCTRIA_GPT_MODEL") or os.getenv("OPENAI_MODEL"),
)

load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / ".env")


# === FastAPI åˆæœŸåŒ– ===
app = FastAPI(title="Noctria LLM Server")

# CORSè¨­å®šï¼ˆGUIãªã©å¤–éƒ¨ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã«å¿…è¦ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªç’°å¢ƒã§ã¯é©å®œåˆ¶é™ã—ã¦ãã ã•ã„
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# === è©•ä¾¡çµæœãƒ‡ãƒ¼ã‚¿æ§‹é€  ===
class StrategyEvaluation(BaseModel):
    filename: str
    final_capital: float
    result: str  # "âœ… æ¡ç”¨" or "âŒ ä¸æ¡ç”¨"


# === ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š ===
EVAL_RESULT_PATH = "/noctria_kingdom/airflow_docker/logs/veritas_eval_result.json"

# === APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ===


@app.get("/")
def read_root():
    return {"message": "ğŸ§  Noctria LLMã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­"}


@app.get("/veritas/eval_results", response_model=List[StrategyEvaluation])
def get_veritas_eval_results():
    """
    Veritas ã«ã‚ˆã‚‹è©•ä¾¡çµæœï¼ˆveritas_eval_result.jsonï¼‰ã‚’è¿”ã™ã€‚
    GUIã‚„ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‹ã‚‰å¯è¦–åŒ–å¯èƒ½ã€‚
    """
    if not os.path.exists(EVAL_RESULT_PATH):
        return []

    with open(EVAL_RESULT_PATH, "r", encoding="utf-8") as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            return []

    filtered = []
    for entry in data:
        if (
            isinstance(entry, dict)
            and "filename" in entry
            and "final_capital" in entry
            and "result" in entry
        ):
            filtered.append(StrategyEvaluation(**entry))
    return filtered


def _log_llm_usage(resp):
    """Best-effort logging of OpenAI-like usage fields."""
    try:
        import logging  # local import safe

        u = getattr(resp, "usage", None)
        if u is not None:
            pt = getattr(u, "prompt_tokens", None)
            ct = getattr(u, "completion_tokens", None)
            tt = getattr(u, "total_tokens", None)
            logging.info("LLM usage prompt=%s completion=%s total=%s", pt, ct, tt)
        else:
            logging.info("LLM usage unavailable (provider?)")
    except Exception as _e:
        logging.exception("LLM usage logging failed: %s", _e)
